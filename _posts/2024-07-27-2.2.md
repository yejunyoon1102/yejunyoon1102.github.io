---
layout: post
title: "머신러닝 : 2.2 데이터 전처리"
date: 2024-07-27
categories: [Machine Learning, 2장]
---
# 2.2 데이터 전처리

<problem : 길이가 25cm이고 무게가 250g이면 도미인데 앞선 모델은 빙어라고 예측한다는데??>

### 넘파이로 데이터 준비하기

```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

```python
# 입력 데이터 만들기

import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))

print(fish_data[ : 5]

>>>
[[ 25.4 242. ]
 [ 26.3 290. ]
 [ 26.5 340. ]
 [ 29.  363. ]
 [ 29.  430. ]]
```

**`np.column_stack()`** : 전달받은 리스트를 일렬로 세운 다음, 차례대로 연결! 

이전에는 타깃 데이터를 만들 때 [0] 이랑 [1]을 여러번 곱했지만…

**`np.ones()`** , **`np.zeros()`** : 원하는 개수의 1과 0을 채운 배열을 만들어줌

```python
# 타깃 데이터(정답) 만들기

fish_target = np.concatenate((np.ones(35), np.zeros(14))

print(fish_target)

>>>
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0.]
```

**`np.concatenate()`** : 첫번째 차원을 따라 배열을 연결함!!

![1.png](/assets/img/posts/ML/ML2.2/1.png)

### 사이킷런으로 훈련 세트와 테스트 세트 나누기

이전에는 넘파이 배열의 인덱스를 직접 섞어서 나누었지만…

**`train_test_split()`** : 전달되는 리스트나 배열을 비율에 맞게 훈련 세트와 테스트 세트로 나누어줌!!

```python
from sklearn.model_selection import train_test_split
```

```python
train_input, test_input, train_target, test_target 
= train_test_split(fish_data, fish_target, random_state = 42)
```

**`random_state`** : 앞서 np.random.seed() 사용한 것 처럼 랜덤 시드 지정하는 매개변수

![2.png](/assets/img/posts/ML/ML2.2/2.png)

기본적으로 75%가 훈련 세트, 25%가 테스트 세트로 split 해줌

```python
print(train_input.shape, test_input.shape)
print(train_target.shape, test_target.shape)

>>>
(36, 2) (13, 2)
(36,) (13,)
```

```python
print(test_target)

>>>
[1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
```

13개 테스트 세트 중에 10개가 도미[1] 이고, 3개가 빙어[0] 이네..?

원래 도미가 35개고 빙어가 14개니까 2.5 : 1 비율로 나와야 하는데… 이 테스트 세트에서는 3.3 : 1 이네.. → 샘플링 편향!!

**`stratify`** 매개변수에 타깃 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눔!!

```python
train_input, test_input, train_target, test_target 
= train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42)
```

### 수상한 도미 한 마리

```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)

>>>
1.0
```

흠..정확도 100%니까 도미 데이터 넣고 예측하면 도미로 예측하겠지?

```python
print(kn.predict([25, 150]))

>>>
[0.]
```

Whattt!?!?!?!?!?!

```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:, 0], train_input[:, 1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![3.png](/assets/img/posts/ML/ML2.2/3.png)

새로운 샘플은 **`marker`** 매개변수를 **`‘^’`** 로 지정하여 삼각형으로 나타냄!

이 샘플은 오른쪽에 뻗어있는 도미 데이터에 더 가까운 것 같은데 왜 빙어 데이터에 가깝다고 판단했나..?

k-최근접 이웃 알고리즘은 주변의 샘플 중에서 다수인 클래스를 예측으로 사용함

**`kneighbors()`** 메소드를 사용해서 이웃까지의 거리와 이웃 샘플의 인덱스를 반환시키자!!(기본적으로 5개의 이웃이 반환됨)

```python
distances, indexes = kn.kneighbors([[25, 150]])
```

```python
plt.scatter(train_input[:, 0], train_input[:, 1])
plt.scatter(25, 150, marker = '^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = 'D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![4.png](/assets/img/posts/ML/ML2.2/4.png)

**`plt.scatter(train_input[indexes,0], train_input[indexes,1], marker = 'D')`** : 가장 가까운 5개의 이웃들을 마름모로 표현함!!

확인해보니 4개가 빙어고 1개가 도미네.. 그러니까 빙어로 예측하지!

```python
print(train_target[indexes])

>>>
[[1. 0. 0. 0. 0.]]
```

```python
print(distances)

>>>
[[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]]
```

### 기준 재설정

가장 가까운 거리가 92고 그 다음이 130이라고..? 눈으로 볼때는 아닌데…

![5.png](/assets/img/posts/ML/ML2.2/5.png)

아… 축간의 거리가 다르구나…범위를 동일하게 설정해야겠네..?

**`xlim()`** : x축 범위 지정

**`ylim()`** : y 축 범위 지정

```python
plt.scatter(train_input[:, 0], train_input[:, 1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim(0, 1000)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![6.png](/assets/img/posts/ML/ML2.2/6.png)

다시 설정해보니까 x축은 딱히 연관이 없네…

두 특성의 값이 놓인 범위가 다를 때!! → **스케일**이 다름!!

데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측 할 수 없음. 알고리즘이 거리 기반일 때 특히!!(k-최근접 이웃 포함)

제대로 사용하려면 특성값을 일정한 기준으로 맞춰야 → **데이터 전처리**

- **표준점수 (z점수)**

각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타냄
이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교 가능!

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_inpit, axis=0)
```

**`np.mean()`** : 평균 계산 , **`np.std()`** : 표준편차 계산

특성마다 값의 스케일이 다르므로 평균과 표준편차는 각 특성별로 계산해야!!

**`axis=0`** 으로 지정함으로써 행을 따라 각 열의 통계값을 계산함

![7.png](/assets/img/posts/ML/ML2.2/7.png)

```python
print(mean,std)

>>>
[ 27.29722222 454.09722222] [  9.98244253 323.29893931]
```

원본데이터에서 평균을 빼고 표준편차로 나누어 표준점수로 변환

```python
train_scaled = (train_input - mean) / std
```

### 전처리 데이터로 모델 훈련하기

```python
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylable('weight')
plt.show()
```

![8.png](/assets/img/posts/ML/ML2.2/8.png)

흠.. 모양이 좀 이상한데..?

훈련세트를 평균으로 빼고 표준편차로 나누어졌기 때문에 예측 데이터도 같이 바꿔줘야 했었구나!!

```python
new = ([25, 150] -mean) / std
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![9.png](/assets/img/posts/ML/ML2.2/9.png)

앞서 산점도랑 비슷하지만 x축과 y축의 범위가 똑같이 -1.5 ~ 1.5로 바뀌었네!! 훈련 데이터의 두 특성이 비슷한 범위를 차지

데이터 전처리가 끝났으니 모델 훈련 재개

```python
# 테스트 세트도 표준점수로 변환

test_scaled = (test_input - mean) / std
kn.fit(train_scaled, train_target)
kn.score(test_scaled, test_target)

>>>
1.0
```

```python
# 새로운 샘플 다시 예측

print(kn.predict([new]))

>>>
[1.]
```

드디어 도미로 예측하네!!

특성을 표준점수로 바꾸었으니 k-최근접 이웃 알고리즘이 올바르게 거리 측정했을듯

```python
distances, indexes = kn.kneighbors([new])
plt.scatter(train_scaled[:, 0], train_scaled[:, 1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show
```

![10.png](/assets/img/posts/ML/ML2.2/10.png)

이웃한 5개 샘플 도미로 확인!!