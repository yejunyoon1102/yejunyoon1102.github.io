---
layout: post
title: "머신러닝 : 3.3 특성 공학과 규제(다중, 릿지, 라쏘 회귀)"
date: 2024-08-02
categories: [Machine Learning, 3장]
---
# 3.3 특성 공학과 규제

<problem : 다항 회귀로 농어의 무게를 어느 정도 예측할 수 있지만, 여전히 훈련 세트보다 테스트 세트의 점수가 높음! → 더 복잡하게 만들어야>

선형 회귀는 특성이 많을수록 효과가 좋으므로 농어의 길이 뿐만 아니라 높이와 두께를 추가하자!!

### 다중 회귀

**다중 회귀** : 여러 개의 특성을 사용한 선형 회귀!

![1.png](/assets/img/posts/ML/ML3.3/1.png)

특성이 1개면 선형 회귀 모델은 직선을 학습!

특성이 2개면 평면을 학습!

특성이 3개 이상이면 n차원 공간(상상 불가)

기존의 특성을 사용해 새로운 특성을 뽑아내는 작업 : **특성공학**

(ex. 농어 길이 x 농어 높이 를 새로운 특성으로 추가)

### 데이터 준비

```python
# 농어 데이터 데이터프레임화 및 넘파이 변환

import pandas as pd
df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()
print(perch_full)
```

```python
import numpy as np
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
     1000.0, 1000.0]
     )
```

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
perch_full, perch_weight, random_state=42)
```

### 사이킷런의 변환기

사이킷런에는 **변환기**라 부르는 특성을 만들거나 전처리하는 클래스가 있음!!

- 예시

```python
# 2개의 특성 2와 3으로 이루어진 샘플 하나 적용

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([2, 3])
print(poly.transform[[2, 3]))

>>>
[[1. 2. 3. 4. 6. 9.]]

poly = PolynomialFeatures(include_bias=False)
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))

>>>
[[2. 3. 4. 6. 9.]]
```

**`fit()`** 메소드는 새롭게 만들 특성 조합을 찾고, **`transform()`** 메소드는 실제 데이터 변환

훈련(fit)을 해야 변환(transform)이 가능함!!!!

변환기는 입력 데이터를 변환하는데 타깃 데이터가 필요하지 않음!!

**`PolynomialFeatures`** 클래스는 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가함

사이킷런은 자동으로 절편값(여기선 1)을 추가하므로 **`include_bias=False`**로 지정하여 없애주자 → 사이킷런 모델은 자동으로 추가된 절편 항을 무시하므로 그냥 해도 됨!!

```python
# 훈련 세트 변환

poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)

>>>
(42, 9)    # 42개의 샘플(행)이 있고 9개의 특성(열)이 있구나!!
```

```python
poly.get_feature_names()

>>>
['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2',
       'x2^2']
```

**`get_feature_names()`** 메소드는 9개의 특성이 각각 어떤 입력의 조합으로 만들어졌는지 알려줌!

**훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 꼭 변환해야!!!**

```python
# 테스트 세트 변환

test_poly = poly.transform(test_input)
```

### 다중 회귀 모델 훈련하기

다중 회귀 모델을 훈련하는 것은 선형 회귀 모델을 훈련하는 것과 같음!!

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target)
print(lr.score(test_poly, test_target)

>>>
0.9903183436982125
0.9714559911594111
```

훈련 세트 점수 > 테스트 세트 점수 : 테스트 점수는 높아지지 않았지만 이전에 농어의 길이만 사용했을 때 있었던  과소적합 문제 해결!!

PolynomialFeatures 클래스의 degree 매개변수를 사용해서 고차항을 더 늘려서 복잡하게 해볼까??

```python
poly = PolynomailFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape)

>>>
(42, 55)           # 42개의 샘플에 55개의 특성!!
```

```python
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target)
print(lr.score(test_poly, test_target)

>>>
0.9999999999996433
-144.40579436844948
```

!?!??!!??!!?!!?!?

특성의 개수를 크게 늘리면 선형 모델이 매우 강력해지므로 훈련 세트에 대해 거의 완벽하게 학습할 수 있지만, 훈련 세트에 너무 과대적합 되므로 테스트 세트에서는 형편없는 점수가 나오는구나!!

### 규제

**규제** : 머신러닝 모델이 훈련 세트에 과대적합 되지 않도록 만듦!

선형 회귀 모델의 경우, 특성에 곱해지는 계수(기울기)의 크기를 작게 만드는 규제

![2.png](/assets/img/posts/ML/ML3.3/2.png)

일반적으로 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기(스케일)이 서로 많이 다르면 안되겠구나!! → **정규화**를 해야겠지?

전에는 z-score을 직접 구했지만 StandardScaler 클래스를 사용하자

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

**`StandardScaler()`**는 각 특성의 평균을 0으로 표준편차를 1로 만드는 정규화 시행!

선형 회귀 모델에 규제를 추가한 모델을 **릿지(ridge)** 와 **라쏘(lasso)** 라고 함!

- 릿지 : 계수를 제곱한 값을 기준으로 규제를 적용
- 라쏘 : 계수의 절댓값을 기준으로 규제를 적용

두 알고리즘 모두 계수의 크기를 줄이지만 라쏘는 아예 0으로 만들수도 있음!

### 릿지 회귀

릿지와 라쏘 모두 **`sklearn.linear_model`** 패키지 안에 존재

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

>>>
0.9896101671037343
0.9790693977615387
```

훈련 세트의 점수가 약간 낮아지고 테스트 세트 점수도 정상으로 돌아옴!!

릿지와 라쏘 모델을 사용할 때 규제의 양을 임의로 조절 할 수 있음!!

**`alpha`** 매개변수 이용!!

alpha 값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고 조금 더 과소적합되도록 유도

alpha 값이 작으면 계수를 줄이는 역할이 줄어들고 과대적합될 가능성 커짐

alpha 값처럼 머신러닝 모델이 학습 할 수 없고 사람이 알려줘야 하는 파라미터를 **하이퍼파라미터**라고 함!

- 적절한 alpha 값을 어떻게 찾지??

→ alpha 값에 대한 R² 값의 그래프를 그려서 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha 값이 됨!!!

```python
# alpha 값을 바꿀 때마다 score() 메소드의 결과를 저장할 리스트 만들기

import matplotlib.pyplot as plt
train score = []
test_score = []
```

```python
# alpha 값을 0.001에서 100까지 10배씩 늘려가며 릿지 회귀 모델 훈련한 후
# 훈련 세트와 테스트 세트의 점수를 파이썬 리스트에 저장

alpha_list = [0.001, 0.001, 0.1, 1, 10, 100]
for alpha in alpha_list
		ridge = Ridge(alpha=alpha)
		ridge.fit(train_scaled, train_target)
		train_score.append(ridge.score(train_scaled, train_target))
		test_score.append(ridge.score(test_scaled, test_target))
```

alpha 값을 0.001에서 10배씩 늘렸기 때문에 이대로 그래프를 그리면 그래프 왼쪽이 너무 촘촘해지므로 로그함수로 바꾸어 표현하자!

```python
plt.plot(np.log10(alpha_list), train_score, label='train set')
plt.plot(np.log10(alpha_list), test_score, label='test set')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()
plt.show()
```

![3.png](/assets/img/posts/ML/ML3.3/3.png)

적절한 alpha 값은 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 -1, 즉 0.1 이네!!

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_sclaed, train_target))
print(ridge.score(test_scaled, test_target))

>>>
0.9903815817570368
0.9827976465386896
```

### 라쏘 회귀

릿지 회귀와 매우 비슷함!! Ridge 클래스를 Lasso 클래스로 바꾸는 것이 전부

```python
from sklearn.Linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

>>>
0.989789897208096
0.9800593698421884
```

```python
# 적절한 alpha 값 찾기

train_score=[]
test_score=[]
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list
		lasso = Lasso(alpha=alpha, max_iter = 10000)
		lasso.fit(train_scaled, train_target)
		train_score.append(lasso.score(train_scaled, train_target))
		test_score.append(lasso.score(test_scaled, test_target))
		
plt.plot(np.log10(alpha_list), train_score, label='train set')
plt.plot(mp.log10(alpha_list), test_score, label='test set')
plt.xlabel('alpha')
plt.ylable('R^2')
plt.legend()
plt.show()
```

![4.png](/assets/img/posts/ML/ML3.3/4.png)

두 그래프가 가장 가깝고 두 세트 모두 점수가 높은 최적의 alpha 값은 1, 즉 10이다!

```python
lasso= Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

>>>
0.9888067471131867
0.9824470598706695
```

앞서 라쏘 모델은 계수 값을 아예 0으로 만들 수 있다고 했었음!

라쏘 모델의 계수는 coef_ 속성에 저장되어 있음

```python
print(np.sum(lasso.coef_ == 0))

>>>
40
```

55개의 특성 중 40개가 0이고 유용한 특성을 15개만 써서 훈련했네!!