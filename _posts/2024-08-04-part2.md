---
layout: post
title: "데이터 분석 : Part.2 데이터 입출력"
date: 2024-08-04
categories: [Data Analysis, Pandas]
---
# 2.1 외부 파일 읽어오기

**판다스 데이터 입출력 도구**

![1.png](/assets/img/posts/PANDAS/part2/1.png)

### csv 파일

```python
pandas.read_csv(파일 경로(이름))
pd.read_csv(파일 경로(이름))
```

read_csv() 함수의 **`header`** 옵션은 데이터프레임의 열 이름으로 사용할 행을 지정

![2.png](/assets/img/posts/PANDAS/part2/2.png)

**`index_col`** 옵션은 데이터프레임의 행 인덱스가 되는 열을 지정함

![3.png](/assets/img/posts/PANDAS/part2/3.png)

header 옵션이 없으면 csv 파일은 첫 행의 데이터가 열 이름이 된다. 한편 index_col 옵션을 지정하지 않으면 행 인덱스를 정수 0, 1, 2가 자동으로 지정 

```python
import pandas as pd

df1 = pd.read_csv(file_path)      # 기본값 header=0
print(df1)

>>>
   c0  c1  c2  c3
0   0   1   4   7
1   1   2   5   8
2   2   3   6   9

df2 = pd.read_csv(file_path, header=None)
print(df2)

>>>
    0   1   2   3
0  c0  c1  c2  c3
1   0   1   4   7
2   1   2   5   8
3   2   3   6   9

df3 = pd.read_csv(file_path, index_col=None)
print(df3)

>>>
   c0  c1  c2  c3
0   0   1   4   7
1   1   2   5   8
2   2   3   6   9

df4 = pd.read_csv(file_path, index_col='c0')
print(df4)

>>>
    c1  c2  c3
c0            
0    1   4   7
1    2   5   8
2    3   6   9
```

**encoding** 옵션 : 텍스트 인코딩 종류 지정 (보통 ‘utf-8’, 한국어 데이터 ‘cp949’)

### Excel 파일

read_csv() 함수와 사용법 거의 비슷!!

```python
pandas.read_excel(파일 경로(이름))
pd.read_excel(파일 경로(이름))
```

xlsx 확장자를 갖는 경우 : engine 옵션에 ‘openpyxl’ 지정

xls 확장자를 갖는 경우 : engine 옵션에 ‘xlrd’ 지정

### JSON 파일

JSON 파일은 데이터 공유를 목적으로 개발된 특수한 파일 형식

파이썬 딕셔너리와 비슷하게 key : value 구조를 갖음

```python
pandas.read_json(파일 경로(이름))
pd.read_json(파일 경로(이름))
```
# 2.2 웹(web)에서 가져오기

### HTML 기초

HTML : 웹사이트의 구조를 표시하기 위한 언어

![4.png](/assets/img/posts/PANDAS/part2/4.png)

![5.png](/assets/img/posts/PANDAS/part2/5.png)

속성(attribute) : 태그의 추가적인 정보, 여러개 혹은 0개 부여할 수 있음

내용에는 텍스트나 태그가 들어 갈 수 있고, 없어도 됨

<body> 태그 안에 실제로 화면에 표시되는 내용이 들어감!!

주석은 <!—주석처리내용—> 으로 달 수 있음

VS Code 에서 그냥 치기만 해도 시작, 종료태그 달아줌

**태그 종류**

- `<title>` : 웹을 띄웠을 때 제목을 나타냄
- `<h1>` : heading 태그, 중요도를 나타냄
- `<div>` : 구역을 나타냄

![6.png](/assets/img/posts/PANDAS/part2/6.png)

- `<p>` : 문단을 나타냄
- `<a>` : 하이퍼링크 태그, `href` : 클릭했을 때 이동하는 주소
- `<input>` : 입력을 담당하는 태그, `placeholder` : 어떤걸 입력?(회색글씨)
- `<button>` : 클릭할 수 있는 버튼 생성
- `<ul>` : 순서가 없는 목록을 만들 때 사용
- `<ol>`: 순서가 있는 목록을 만들 때 사용
- `<li>` : `<ul>` 또는 `<ol>` 태그 안에서 사용, 목록 항목을 나타냄

```python
<!DOCTYPE html>
<html>
    <head>
        <!--문서의 잡다한 정보-->
        <title>스타트핏: 운동의 시작</title>
    </head>
    <body>
        <!--화면에 표시되는 내용-->
        <h1>스타트핏: 운동의 시작</h1>

        <div>
            <h1>기초 체력업! 무분할 루틴</h1>
            <p>운동을 처음 시작하는 헬린이 모여</p>
            <a href="https://www.naver.com">지금 시작하기</a>
        </div>

        <div>
            <h1>기초 체력업! 3분할 루틴</h1>
            <p>운종 좀 했니? 드루와</p>
            <a href="https://www.naver.com">지금 시작하기</a>
        </div>

        <div>
            <h1>나만의 운동 목표 설정</h1>
            <input type="text" placeholder="목표입력">
            <button onclick="alert('레츠기릿!')">저장하기</button>
            <ul>
                <li><a href="#">웨이트로 몸짱되기!</a></li>
                <li><a href="#">유산소로 다이어트!</a></li>
                <li><a href="#">필라테스로 유연성과 코어잡기</a></li>
            </ul>
        </div>
    </body>
</html>
```

![7.png](/assets/img/posts/PANDAS/part2/7.png)

### CSS

![8.png](/assets/img/posts/PANDAS/part2/8.png)

→ 페이지 안에 있는 모든 h1 태그에 대해 글자 색깔을 빨강으로 바꿔!!

**선택자(selector)**

웹페이지에서 원하는 태그를 선택하는 문법

- 태그 선택자

태그 이름으로 선택하는 것

보통 다른 선택자와 결합하여 사용됨

```python
<body>
			<h1>Pick This</h1>
			<p>Pick This, Too</p>
</body>

>>>
h1 {color : red;}
p {color : blue;}           # 웹 페이지의 h1태그와 p태그에 대해 수행!
```

- **클래스 선택자**

클래스 속성 값으로 선택하는 것

클래스 : 태그에 별명을 주는 것

클래스는 .(마침표) 로 시작!!

```python
# 예제(2,3번째 <p>만 골라서 바꾸고 싶은데..)

<body>
			<p>Do not pick this</p>
			<p class="large" >Pick this</p>
			<p class="large" >Pick this</p>
</body>

>>>
**.large** {               # 웹페이지에 있는 태그 중 class 값이 large 인것만 뽑겠다!!               
	font-size : 50px;
	font_weight : 600;
}
```

- 아이디 선택자

아이디 속성 값으로 선택하는 것

아이디 : 태그에 별명을 주는 것

아이디는 #(샵)으로 시작!!

```python
<body>
			<p id="title">Do not pick this</p>
			<p>Pick this</p>
			<p>Pick this</p>
</body>

>>>
#title {
	font-size : 32px;
	font-decoration : underline;
}
```

- **자식 선택자**

바로 아래 자식태그를 선택하는 것

내가 원하는 태그에 별명이 없을 때 사용

```python
<body>
			<div>
					<p>Pick This</p>
					<p>Pick This</p>
			</div>
			<p>Do not pick this</p>
</body>

>>>
div > p {font-size : 30px }

<body>
			<div class="header">
					<p>Pick This</p>
					<p>Pick This</p>
			</div>
			<div class="section">
					<p>Do not Pick This</p>
					<p>Do not Pick This</p>
			</div>
<body/>

>>>
.header > p {font-size : 30px;}        # 부모태그를 header 라는 클래스!
```

### 웹 스크래핑

```python
pandas.read_html('웹 주소(URL)' 또는 'HTML 파일 경로(이름)')
pd.read_html('웹 주소(URL)' 또는 'HTML 파일 경로(이름)')
```

read_html() 함수는 HTML 웹 페이지에 있는 <table> 태그에서 표 형식의 데이터를 모두 찾아서 데이터프레임으로 변환

![9.png](/assets/img/posts/PANDAS/part2/9.png)

```python
# 모든 테이블을 표시하고 어떤 테이블을 선택할 지 확인

for i in range(len(tables)):
    print(f"tables[{i}]")
    print(tables[i])
    print('\n')
```

웹 스크래핑(Web Scraping) : 특정 웹페이지에서 원하는 정보를 추출하여 데이터베이스에 저장하거나 분석에 사용

웹 크롤링(Web Crawling) : 프로그램이 웹사이트를 정기적으로 돌며 정보를 추출하는 기술

### 웹 크롤링 기초

정적 페이지(static page) 크롤링 : 데이터의 추가적인 변경이 일어나지 않는 페이지

- 데이터 받아오기(requests 라이브러리)

파이썬에서 서버에 요청을 보내고 응답받기

HTTP 통신으로 HTML 받아오기

- 데이터 뽑아내기(BeautifulSoup4 라이브러리)

HTML에서 원하는 부분만 추출

CSS 선택자를 잘 만드는 것이 핵심!!

**requests, BeutifulSoup 라이브러리 기본 사용법**

```python
# 라이브러리 설치

pip install requests
pip install bs4

import requests
response = requests.get("https://startcoding.pythonanywhere.com/basic")
response.status_code

>>>
200

response.text           # 웹페이지 텍스트 문자열 형태로 긁어오기
```

**`requests.get`** 으로 웹페이지 긁어오고 **`status_code`** 확인해서 200 이 나오면 잘 연결되고 있다는 뜻!!

```python
# 기본 세팅!!!

import requests
from bs4 import BeautifulSoup

response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')
```

원하는 웹페이지 들어가서 F12로 구글 개발자 모드 활성화!

![10.png](/assets/img/posts/PANDAS/part2/10.png)

왼쪽 위 대각선 화살표 모양 클릭 후 데이터 뽑아낼 곳 클릭하면 html 코드 위치로 가져다줌, 원하는 클래스 클릭하면 속성값 복사 가능!

클래스 이름 복사 후 Ctrl+F 누르면 선택자 검색 기능 활성화!

.클래스이름 넣으면 몇 개 있는지 검색 가능

```python

# select_one : 매칭되는 태그 중 첫번째 반환

soup.select_one(."brand-name")        
>>>
<a class="brand-name" 
href="https://www.youtube.com/channel/UCHwhZ7HPBhUh2IscPSL0pHA" 
target="_blank"><i class="fa fa-youtube"></i>스타트코딩</a>

soup.select_one(".brand-name").text
>>>
'스타트코딩'

soup.select_one(".brand-name").attrs['href']
>>>
'https://www.youtube.com/channel/UCHwhZ7HPBhUh2IscPSL0pHA'

soup.select_one(".brand-name").attrs['target']
>>>
'_blank'

```

### 웹 크롤링 실전

**한 상품만 크롤링**

```python
# 기본 라이브러리 & 세팅

import requests
from bs4 import BeautifulSoup

response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')

# 카테고리 크롤링

soup.select_one(".product-category").text

>>>
'노트북'

# 자식 태그 이용 크롤링!!

soup.select_one(".product-name > a").text

>>>
'에이서 스위프트 GO 16 OLED, 스틸 그레이, 코어i7, 512GB, 16GB, WIN11 Home, SFG16-71-77FT'

# 뒤에 줄바꿈 문자, 탭 제거하기

soup.select_one(".product-price").text

>>>
'1,419,000원\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t'

soup.select_one(".product-price").text.strip()

>>>
'1,419,000원'

soup.select_one(".product-price").text.strip().replace(' , ' , '').replace('원', '')

>>>
1419000
```

**`strip()`** : 앞 뒤 공백 제거

**`replace(’변경 전 문자’ , ‘변경 후 문자’)`** : 문자열 교체

**여러 상품 크롤링**

숲 : 페이지 전체 HTML / 나무: 원하는 정보를 모두 담는 태그

1. 숲에서 원하는 정보를 모두 담고 있는 나무를 찾는다
2. CSS 선택자를 만들어 테스트한다
3. soup.select(”CSS선택자”) 로 숲에서 나무들을 뽑는다
    
    **`select()`** : 선택자에 매칭되는 태그 전체를 리스트로 반환
    
4. 반복문을 돌면서 나무에서 하나씩 열매를 추출한다

<나무 태그 찾는 법>

1. 태그 하나를 찾는다
2. 상위태그(부모태그)를 찾아 올라간다
3. 원하는 정보를 모두 담고 있는지 확인한다

```python
import requests
from bs4 import BeautifulSoup

response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')
items = soup.select(".product")

for item in items:
    category = item.select_one(".product-category").text
    name = item.select_one(".product-name").text
    link = item.select_one(".product-name > a").text
    price = item.select_one(".product-price").text.split('원')[0].replace(',', '')
    print(category, name, link, price)

```

**`split(’n’)`** : ‘n’ 기준으로 앞/뒤 를 쪼갠다!

**여러 페이지 크롤링**

URL(Uniform Resource Locator)

-인터넷 주소 형식

-Protocol - Domain - Path - Parameter

![11.png](/assets/img/posts/PANDAS/part2/11.png)

파라미터 : 서버에 추가 정보 제공

**페이징 알고리즘**

1. 페이지를 바꾸면서 URL이 변경되는 부분을 찾는다
2. 페이지를 증가시키면서 요청을 보낸다

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

for i in range(1, 5):
    response = requests.get(f"https://startcoding.pythonanywhere.com/basic?page={i}")
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    items = soup.select(".product")

    for item in items:
        category = item.select_one(".product-category").text
        name = item.select_one(".product-name").text
        link = item.select_one(".product-name > a").text
        price = item.select_one(".product-price").text.split('원')[0].replace(',', '')
        print(category, name, link, price)

```

**데이터 엑셀에 저장하는 법**

1. 비어있는 리스트를 만들고 데이터를 한 행씩 추가한다

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

data = []
for i in range(1, 5):
    response = requests.get(f"https://startcoding.pythonanywhere.com/basic?page={i}")
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    items = soup.select(".product")

    for item in items:
        category = item.select_one(".product-category").text
        name = item.select_one(".product-name").text
        link = item.select_one(".product-name > a").text
        price = item.select_one(".product-price").text.split('원')[0].replace(',', '')
        print(category, name, link, price)
        data.append([category, name, link, price])

```

1. 데이터프레임을 만들고 엑셀로 저장한다

```python
# 데이터프레임 만들기

df = pd.DataFrame(data, columns=['카테고리', '상품명', '상세페이지링크', '가격'])

# 엑셀 저장

df.to_excel('result.xlsx', index=False)
```

**`index=False`** 옵션 이용해서 앞에 정수형 인덱스 숫자 제거
# 2.4 데이터 저장하기

### csv 파일로 저장

```python
df.to_csv("파일 이름(경로)")
```

### JSON 파일로 저장

```python
df.to_json("파일 이름(경로)")
```

### Excel 파일로 저장

```python
df.to_excel("파일 이름(경로)")
```

### 여러 개의 데이터프레임을 하나의 Excel 파일로 저장

```python
pandas.ExcelWriter("파일 이름(경로)")
```

ExcelWriter() 함수는 Excel 워크북 객체 생성

데이터프레임에 to_excel() 메소드를 적용할 때 삽입하려는 워크북 객체(Excel 파일)을 인자로 전달함

sheet_name 옵션에 Excel 파일의 시트 이름을 입력하여 삽입되는 시트 위치를 지정할 수 있음

(df1, df2 만들었다고 가정)

```python
# df1을 'sheet1'으로, df2를 'sheet2'로 저장 (엑셀파일명은 "df_excelwriter.xlsx")
with pd.ExcelWriter("./data/df_excelwriter.xlsx") as writer:
    df1.to_excel(writer, sheet_name="sheet1")
    df2.to_excel(writer, sheet_name="sheet2")
```

![12.png](/assets/img/posts/PANDAS/part2/12.png)