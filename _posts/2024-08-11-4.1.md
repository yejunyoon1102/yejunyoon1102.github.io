---
layout: post
title: "머신러닝 : 4.1 로지스틱 회귀"
date: 2024-08-11
categories: [Machine Learning, 4장]
---
# 4.1 로지스틱 회귀

< problem : 럭키백에 포함된 생선의 확률을 알려서 판매할건데… 어떻게 생선의 확률을 구하지..? >

### 럭키백의 확률

럭키백에 들어갈 수 있는 생선은 7개

→ 럭키백에 들어간 생선의 크기, 무게 등이 주어졌을 때 7개 생선에 대한 확률 출력

확률은 숫자니까 회귀인가…. 7개 생선에 대한 분류인가….

→ K-최근접 이웃은 주변 이웃을 찾아주니까 이웃의 클래스 비율을 확률이라고 출력!?!?

![1.png](/assets/img/posts/ML/ML4.1/1.png)

### 데이터 준비하기

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```

![2.png](/assets/img/posts/ML/ML4.1/2.png)

```python
# 어떠한 생선이 있는지 고유한 값 추출

print(pd.unique(fish['Species']))

>>>
['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
```

Species 열을 타깃으로 만들고 나머지 5개 열은 입력 데이터로 사용 → 데이터 프레임에서 원하는 열을 리스트로 나열!!!

```python
# 입력데이터 설정, 넘파이 배열

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()

print(fish_input[:5])

>>>
[[242.      25.4     30.      11.52     4.02  ]
 [290.      26.3     31.2     12.48     4.3056]
 [340.      26.5     31.1     12.3778   4.6961]
 [363.      29.      33.5     12.73     4.4555]
 [430.      29.      34.      12.444    5.134 ]]
```

```python
# 타깃 데이터 설정

fish_target = fish['Species'].to_numpy()

# 훈련 세트, 테스트 세트 구분

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
    
# 훈련 세트와 테스트 세트를 표준화 전처리

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### k-최근접 이웃 분류기의 확률 예측

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

>>>
0.8907563025210085
0.85
```

타깃 데이터를 만들 때 fish[Spiecies]를 사용해 만들었기 때문에 훈련 세트와 테스트 세트의 타깃 데이터에도 7개의 생선 종류가 들어가 있음 → 타깃 데이터에 2개 이상의 클래스가 포함된 문제를 **다중 분류** 라고 함

이전 분류를 사용했을 때는 0과 1로 지정하여 타깃 데이터를 만들었는데…

사이킷런에서는 편리하게도 문자열로 된 타깃값 그대로 사용 가능

주의 : 타깃값을 그대로 사이킷런 모델에 전달하면 순서가 자동으로 알파벳 순으로 매겨짐!!

```python
# 정렬된 타깃값 속성 확인

print(kn.classes_)

>>>
['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
```

```python
# 테스트 세트의 샘플 처음 5개 타깃값 예측

print(kn.predict(test_scaled[:5]))

>>>
['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
```

- 예측 동작 원리

사이킷런의 분류 모델은 **`predict_proba()`** 메소드로 클래스별 확률값을 반환

```python
import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))

>>>
[[0.     0.     1.     0.     0.     0.     0.    ]
 [0.     0.     0.     0.     0.     1.     0.    ]
 [0.     0.     0.     1.     0.     0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
```

predict_proba() 출력 순서는 **`classes_`** 속성과 같음!!

즉, 첫 번째 열이 Bream, 두 번째 열이 Parkki, 세 번째 열이 Perch …

생각해보니 3개의 이웃을 사용하니까 가능한 확률은 0/3, 1/3, 2/3, 3/3 이 전분데…

### 로지스틱 회귀

**로지스틱 회귀**는 이름은 회귀이지만 **분류** 모델!!

선형 회귀와 동일하게 선형 방정식을 학습함

ex)

![3.png](/assets/img/posts/ML/ML4.1/3.png)

a, b, c, d, e는 가중치 혹은 계수

근데 우리가 구해야 되는 거는 확률인데…? → z가 아주 큰 음수일 때 0, 아주 큰 양수일 때 1

**시그모이드 함수(로지스틱 함수)**를 사용하면 가능!!!

![4.png](/assets/img/posts/ML/ML4.1/4.png)

```python
# 넘파이 사용해서 시그모이드 함수 계산

import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```

![5.png](/assets/img/posts/ML/ML4.1/5.png)

### 로지스틱 회귀로 이진 분류 수행하기

**불리언 인덱싱** : 넘파이 배열에서 True, False 값을 전달하여 행을 선택함

```python
# 예시(A, C 선택)

char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

>>>
['A' 'C']
```

비교 연산자 : **`train_target == ‘Bream’`** → train_target 배열에서 ‘Bream’ 인 것은 True 이고 그 외는 모두 False

이후 **비트 OR 연산자 ( | )** 로 합침 : 어느 한 배열에서라도 True 가 있으면 True 반환, 둘 다 False 인 경우에만 False 반환

```python
# 훈련 세트에서 도미(Bream) 와 빙어(Smelt) 행 골라내기

bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

로지스틱 회귀 모델은 sklearn.linear_model 패키지 안에 LogisticRegression 클래스 존재!

```python
# 로지스틱 회귀 모델 훈련

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```

```python
# 처음 5개 샘플 예측

print(lr.predict(train_bream_smelt[:5]))

>>>
['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
```

```python
# 클래스 속성 확인

print(lr.classes_)

>>>
['Bream' 'Smelt']             # 첫 번째 값이 음성 클래스(0)
                              # 두 번째 값이 양성 클래스(1) -> 이진 분류!

# 처음 5개 샘플 예측 확률 

print(lr.predict_proba(train_bream_smelt[:5]))

>>>
[[0.99759855 0.00240145]
 [0.02735183 0.97264817]
 [0.99486072 0.00513928]
 [0.98584202 0.01415798]
 [0.99767269 0.00232731]]
```

첫 번째 열이 Bream일 확률이고, 두 번째 열이 Smelt 일 확률이네!!

```python
# 로지스틱 회귀가 학습한 계수 확인

print(lr.coef_, lr.intercept_)

>>>
[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
```

![6.png](/assets/img/posts/ML/ML4.1/6.png)

LogisticRegression 클래스는 **`decision_function()`** 메소드로 z값 출력 가능!!

```python
# 처음 5개 샘플의 z값 출력

decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

>>>
[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
```

이제 이 z값을 시그모이드 함수에 통과시키면 확률을 얻을 수 있음!!!

**사이파이(scipy)** 라이브러리 안에 **시그모이드 함수**인 **expit()** 사용, np.exp() 사용해서 분수 계산 하는 것보다 편리하고 안전함!!

```python
# z값 시그모이드 함수에 통과시켜 확률 구하기

from scipy.special import expit

print(expit(decisions))

>>>
[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
```

결과를 확인하니 predict_proba() 메서드 출력의 두 번째 열의 값과 동일

→ **`decision_function()`** 메소드는 **양성 클래스**에 대한 z값을 반환!!!!

### 로지스틱 회귀로 다중 분류 수행하기

- LogisticRegression 클래스는 기본적으로 반복적 알고리즘 수행

(max_iter 매개변수에서 반복 횟수 지정, 기본값 100)

- 기본적으로 릿지 회귀와 같이 제곱을 규제!! (L2규제)

릿지 회귀에서는 alpha 매개변수로 규제의 양을 조절했다면, 로지스틱 회귀에서는 C 매개변수로 조절!! , C는 alpha와 반대로 작을수록 규제가 커짐(기본값 1)

```python
# 로지스틱 회귀로 다중 분류 모델 훈련

lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

>>>
0.9327731092436975
0.925
```

```python
# 테스트 세트의 처음 5개 샘플에 대한 예측 출력

print(lr.predict(test_scaled[:5]))

>>>
['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
```

```python
# 테스트 세트의 처음 5개 샘플에 대한 예측 확률 출력

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))          # 소수점 넷째자리 반올림

>>>
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]

```

```python
# 클래스 속성 확인

print(lr.classes_)

>>>

['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
```

- 이진 분류는 샘플마다 2개의 확률을 출력함!!
- 다중 분류는 샘플마다 클래스 개수만큼 확률을 출력함!!

```python
# 계수의 크기 확인

print(lr.coef_.shape, lr.intercept_.shape)

>>>
(7, 5) (7,)
```

7행 5열의 배열이네..? 

z를 7개 계산하고, 특성이 5개니까 7행 5열 이구나!!

→ **다중 분류는 클래스마다 z값을 하나씩 계산!!!!**

가장 높은 z값을 출력하는 클래스가 예측 클래스가 됨

- **이진 분류**는 **시그모이드** **함수**를 통해 z를 0 과 1 사이의 확률값으로 변환

→ 하나의 선형 방정식의 출력값을 0~1 사이로 압축

- **다중 분류**는 **소프트맥스 함수**를 통해 z를 0과 1 사이의 확률값으로 변환

→ 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축

![7.png](/assets/img/posts/ML/ML4.1/7.png)

s1 에서 s7 까지 모두 더하면 1이 되는 원리!!(확률 값과 동일)

```python
# 처음 5개 샘플의 z값 출력

decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))

>>>
[[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
 [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
 [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
 [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
 [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
```

```python
# 소프트맥스 함수를 통해 확률값으로 변환

from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))

>>>
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
```

**`axis =1`**  (행 단위 연산) 로 지정하여 각 행, 즉 각 샘플에 대해 소프트맥스를 계산함

WHY? ) 우리가 원하는 것은 어떠한 샘플이 각 클래스에 속할 확률 계산

→ 이 데이터프레임 에서는 한 행이 하나의 샘플