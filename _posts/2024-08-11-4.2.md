---
layout: post
title: "머신러닝: 4.2 확률적 경사 하강법"
date: 2024-08-11
categories: [Machine Learning, 4장]
---
# 4.2 확률적 경사 하강법

< problem : 매주 7개의 생선 중에서 무작위로 골라 머신러닝 모델을 학습할 수 있게 훈련 데이터를 제공하지만, 샘플을 가지고 있지 않고, 어느 생선이 먼저 올지, 모든 생선이 도착할 때 까지 기다릴 수 없음!! >

### 점진적인 학습

훈련 데이터가 한번에 준비되는 것이 아니라 조금씩 전달되는게 문제…

- 기존의 훈련 데이터에 새로운 데이터를 추가하여 매일매일 훈련시킬까..?

→ 시간이 지날수록 데이터가 늘어나니까…. 서비비용이 말이 안되겠는데!?!?

- 새로운 데이터를 추가할 때 이전 데이터를 버리면서 크기 유지시킬까…?

→ 버리는 데이터에 중요한 데이터가 있으면 어떡하지!?!?

- 버리지 않고 새로운 데이터에 대해서만 조금씪 훈련시킬까…?

→ **점진적 학습(온라인 학습)**

대표적인 점진적 학습 알고리즘은 **확률적 경사 하강법**!!

### 확률적 경사 하강법

확률적 = ‘무작위하게’ , ‘랜덤하게’

경사 = ‘기울기’

하강법 = ‘내려가는 방법’

→ 가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 방법!!!

- **확률적 경사 하강법**

 훈련 세트에서 랜덤하게 하나의 샘플을 선택해서 가파른 경사를 조금씩 내려감, 전체 샘플을 모두 사용할 때까지 계속, 훈련 세트를 모두 사용(**에포크**)하면 다시 처음부터 시작

- **미니배치 경사 하강법**

1개씩 말고 무작위로 몇 개의 샘플을 선택해서 경사를 따라 내려감

- **배치 경사 하강법**

극단적으로 한번 경사로를 따라 이동하기 위해 전체 샘플을 사용

![1.png](/assets/img/posts/ML/ML4.2/1.png)

훈련 세트를 사용해 산 아래에 있는 최적의 장소로 조금씩 이동하는 알고리즘!!

이 때문에 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트되어도 학습을 계속 할 수 있는 것!!(산꼭대기부터 다시 시작할 필요 X)

(신경망 모델에서는 확률적 경사 하강법이나 미니배치 경사 하강법 항상 이용!!!)

→ 어디서 내려가야 되는거지..? 산이 뭐지…? : 손실함수

### 손실함수

**손실함수** : 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준

손실함수의 값이 작을수록 좋지만 어떤 값이 최솟값인지 알지 못하기 때문에 가능한 많이 찾아보고 만족할만한 수준이면 산을 다 내려왔다고 인정

→ 이 값을 찾아서 조금씩 이동하려면 확률적 경사 하강법!!

정확도가 손실함수가 될 수 있지 않을까..??

NO!! 정확도는 연속적이지 않음!! (n/샘플수) 기 때문에 연속적인 함수가 아님!!

![2.png](/assets/img/posts/ML/ML4.2/2.png)

연속적인 손실함수를 만들려면?? → 로지스틱 손실 함수

### 로지스틱 손실 함수

![3.png](/assets/img/posts/ML/ML4.2/3.png)

- 양성 클래스(타깃 = 1) 일때 확률이 1에서 멀어질수록 손실은 아주 큰 음수
- 음성 클래스(타깃=0)일때 확률이 0에서 멀어질수록 손실은 아주 큰 양수

→ **로지스틱 손실 함수(이진 크로스엔트로피 손실 함수)** : 로지스틱 회귀 모델

다중 분류에서는 크로스엔트로피 손실 함수

### SGDClassifier

```python
# 데이터 불러오기

import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
```

```python
# Species 열(타깃 데이터)을 제외한 나머지 5개를 입력 데이터로 사용

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```

```python
# 훈현 세트와 테스트 세트로 나누기

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

```python
# 훈련 세트와 테스트 세트의 특성을 표준화 전처리
# 주의) 훈련 세트에서 학습한 통계값으로 테스트 세트도 변환해야!!!

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

확률적 경사 하강법을 제공하는 대표적 분류용 클래스 **`SGDClassifier`**

```python
# 확률적 경사 하강법 클래스 불러오기

from sklearn.linear_model import SGDClassifier
```

**`loss`** 옵션은 손실함수의 종류를 지정!! **`loss=’log_loss’`** 로 **로지스틱 손실 함수** 지정

max_iter 옵션으로 수행할 에포크 횟수 지정

```python
# 훈련시키기 & 점수 확인

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

>>>
0.773109243697479
0.775
```

확률적 경사 하강법은 점진적 학습이 가능!!

모델을 이어서 훈련할 때 쓰는 partial_fit() 메소드 사용

→ fit() 메소드랑 사용법은 같지만, 호출할 때마다 1 에포크씩 이어서 훈련함

```python
# 1에포크 이어서 룬련

sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

>>>
0.8151260504201681
0.85
```

정확도가 향상되긴 하네..! 근데 얼만큼 더 훈련시켜야 되는거지..?

### 에포크와 과대/과소 적합

적은 에포크 횟수로 훈련한 모델은 훈련 세트와 테스트 세트에 잘 맞지 않는 과소적합일 확률 높음

많은 에포크 횟수로 훈련한 모델은 훈련 세트에 너무 잘 맞아 테스트 세트에는 오히여 점수가 낮은 과대적합일 확률 높음

![4.png](/assets/img/posts/ML/ML4.2/4.png)

따라서 과대적합이 시작되기 전에 훈련을 멈춰야!!  → **조기종료**

```python
# 에포크마다 훈련 세트와 테스트 세트에 대한 점수를 기록하기 위한 리스트
# np.unique() 함수로 train_target 에 있는 7개 생선의 목록 만듦

import numpy as np

sc = SGDClassifier(loss='log_loss', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)
```

```python
# 300번 에포크 훈련 실행, 점수 업데이트

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)

    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

**`classes=classes`** 는 모든 가능한 클래스 레이블 지정!! , 새로운 데이터를 처음 학습할 때 각 클래스가 어떤 것인지 알 수 있도록 함

```python
# 점수 그래프 시각화

import matplotlib.pyplot as plt

plt.plot(train_score, label='train set')
plt.plot(test_score, label='test set')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()
plt.show()
```

![5.png](/assets/img/posts/ML/ML4.2/5.png)

초기에는 두 세트 모두 점수가 낮고, 백 번째 에포크에서 둘 간의 차이가 낮으며. 후에는 차이가 벌어짐

→ 백 번째 에포크가 적절한 반복횟수겠구나!!!

```python
# 에포크 100회로 지정, 다시 훈련시키고 점수 다시 출력

sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

>>>
0.957983193277311
0.925
```

SGDClassifier는 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춤

**`tol`** 매개변수에 향상될 최솟값 지정, **`None`**으로 지정하여 자동으로 멈추지 않고 무조건 100회 반복

여기까지는 확률적 경사 하강법을 사용한 **분류** 모델!!

**회귀** 모델 사용하려면 **`SGDRegressor`** 클래스 사용!!, 사용법은 동일

```python
# 힌지 손실을 사용해 모델 훈련

sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

>>>
0.9495798319327731
0.925
```

**`loss`** 매개변수의 기본값은 **`hinge`** 임 !!!

**힌지 손실**은 **서포트 벡터 머신**이라 부르는 다른 머신러닝 알고리즘을 위한 손실 함수