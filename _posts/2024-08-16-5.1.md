---
layout: post
title: "머신러닝 : 5.1 결정 트리"
date: 2024-08-16
categories: [Machine Learning, 5장]
---
# 5.1 결정 트리

<problem : 캔에 인쇄된 알코올 도수, 당도, pH 값으로 와인 종류를 구별할 수 있는 방법?? → 로지스틱 회귀 써볼까..? >

### 로지스틱 회귀로 와인 분류하기

```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
wine.head()
```

![1.png](/assets/img/posts/ML/ML5.1/1.png)

class 값 : 0이면 레드 와인, 1이면 화이트 와인

→ 레드 와인이랑 화이트 와인을 구분하느 **이진 분류**!!, 화이트 와인이 양성 클래스

**`info()`** : 데이터프레임의 각 열의 데이터 타입과 누락된 데이터가 있는지 확인

```python
# info() 메소드 사용해서 정보 확인

wine.info()
```

![2.png](/assets/img/posts/ML/ML5.1/2.png)

총 6497개의 샘플, 4개의 열은 실수(float)값, Non-Null Count가 6497이므로 누락된 값 X

**`describe()`** : 열에 대한 간략한 통계 정보 출력

```python
# describe() 메소드 사용해서 정보 확인

wine.describe()
```

![3.png](/assets/img/posts/ML/ML5.1/3.png)

→ 도수, 당도 pH값의 스케일이 다르구나..!

```python
# 도수, 당도, pH값 넘파이 배열로 바꾸고, class 열을 target 배열에 저장

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

```python
# 훈련 세트와 테스트 세트로 나누기

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
```

**`test_size=0.2`** : 기본으로 25%를 테스트 세트로 나누지만, 20%만 테스트 세트로 나누겠다!

```python
# 훈련 세트와 테스트 세트의 크기 확인

print(train_input.shape, test_input.shape)

>>>
(5197, 3) (1300, 3)
```

음… 5197개의 훈련 세트와 1300개의 테스트 세트가 2차원 배열로 있구나…

```python
# 데이터 표준화

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

```python
# 로지스틱 회귀 모델 훈련시키기

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

>>>
0.7808350971714451
0.7776923076923077
```

점수가 둘 다 낮은 과소적합이네… 

### 설명하기 쉬운 모델과 어려운 모델

```python
# 로지스틱 회귀가 학습한 계수 확인

print(lr.coef_, lr.intercept_)

>>>
[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]
```

흠… 그러면 보고서를 쓸 때 어떻게 써야하나..?

![4.png](/assets/img/posts/ML/ML5.1/4.png)

이해가 안되겠는걸.. 더 쉽게 설명할 수 있는 모델이 있으려나??

### 결정 트리

스무 고개 스타일!!! 질문을 하나씩 던져가면서 정답과 맞춰가는 느낌

![5.png](/assets/img/posts/ML/ML5.1/5.png)

사이킷런의 **`DecisionTreeClassifier`** 클래스를 사용해 결정 트리 모델 훈련!!

(결정 트리 알고리즘은 최적의 분할을 찾기 전에 특성의 순서를 섞음, 여기서는 결과의 동일성을 위해 랜덤변수 지정하지만, 실전에는 필요 X)

```python
# 결정 트리 모델 훈련시키기

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

>>>
0.996921300750433
0.8592307692307692
```

오 훈련 세트에 대한 점수가 엄청 높고 테스트 세트는 조금 낮은 과대적합이네!!

그림으로 한번 봐볼까..?

plot_tree() 함수를 사용해 결정 트리를 이해하기 쉬운 그림으로 출력!!

```python
# 결정 트리 모델 시각화

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```

![6.png](/assets/img/posts/ML/ML5.1/6.png)

결정 트리는 위에서부터 아래로 거꾸로 자라남!!

맨 위의 노드를 **루트 노드**, 맨 아래 끝에 노드를 **리프 노드** 라고 함

근데 좀 복잡하니까 트리의 깊이를 제한해보자..!

- **`plot_tree()`** 메소드 매개변수

**`max_depth`** : 루트 노드를 제외하고 몇 개의 노드를 확장하여 그리는가?

**`filled`** : 클래스에 맞게 노드 색깔 칠함

**`feature_names`** : 특성의 이름 전달

```python
# 결정 트리 모델 시각화(developed)

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![7.png](/assets/img/posts/ML/ML5.1/7.png)

- 해석 방법

![8.png](/assets/img/posts/ML/ML5.1/8.png)

왼쪽이 조건 만족 (Yes) , 오른쪽이 조건 불만족 (No)

value 값에 있는 왼쪽이 음성(0)클래스 / 오른쪽이 양성(1) 클래스!!!

루트 노트에서 왼쪽으로는 2922개, 오른쪽으로는 2275개 갔네!!

**`filled=True`**로 지정하면 어떤 클래스의 비율이 높아지면 점점 진한색으로 표시되네!!

결정 트리 예측 방법 : **리프 노드**에서 가장 많은 클래스가 예측 클래스!!!!

근데 gini는 뭐지..?

### 불순도

**gini** 는 **지니 불순도**를 의미함!!(criterion 매개변수의 기본값)

→ 노드를 분할하는 기준으로 사용되는 지표 중 하나, 한 노드에 속한 샘플들이 얼마나 섞여 있는지를 나타냄

![9.png](/assets/img/posts/ML/ML5.1/9.png)

지니 불순도가 0이 되는 노드 : 순수 노드

결정 트리 모델은 **부모 노드와 자식 노드의 불순도 차이(정보 이득)가 가능한 크도록** 트리를 성장!!

![10.png](/assets/img/posts/ML/ML5.1/10.png)

**`criterion=’entropy’`**를 지정하면 **엔트로피 불순도** 사용 가능

### 가지치기

결정트리에서 **가지치기**를 하지 않으면 무작정 끝까지 자라나는 트리 생성

→ 훈련 세트에는 아주 잘 맞겠지만 테스트 세트에서 점수 낮음

HOW?

**`DecisionTreeClassifier`** 클래스의 **`max_depth`** 매개변수 이용!! (루트 노드 아래로 n개의 노드까지만 성장시킬 수 있음)

```python
# max_depth 3으로 지정해서 3개의 노드까지만 성장시키기

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

>>>
0.8454877814123533
0.8415384615384616
```

어라? 훈련 세트의 성능은 낮아지고 테스트 세트는 그대로네..?

```python
# plot_tree() 함수로 시각화

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![11.png](/assets/img/posts/ML/ML5.1/11.png)

루트 노드 다음에 있는 깊이 1 의 노드는 당도(sugar)을 기준으로 훈련 세트 나누네

깊이 2 에서는 맨 왼쪽만 당도고 도수, pH를 사용하네?

깊이 3 의 리프 노드에서는 왼쪽에서 세 번째 노드만 음성 클래스가 많네? 여기에 도착해야만 레드와인(0)으로 예측하겠구나..!

그러면 당도는 -0.239보다 작고 또 -0.802보다 작은 데다가 도수는 0.454보다 작아야 되는거네

근데 당도가 음수가 있나..?

**결정 트리 알고리즘에는 표준화 전처리 과정이 필요 없구나!!!!**

```python
# 전처리 하지 않고 결정 트리 모델 훈련시키기

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

>>>
0.8454877814123533
0.8415384615384616
```

```python
# 결정 트리 시각화

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![12.png](/assets/img/posts/ML/ML5.1/12.png)

당도가 1.625와 같거나 작은 와인 중에 알코올 도수가 11.025와 같거나 작은 것이 레드 와인!!

결정 트리는 어떤 특성이 가장 유용한지 나타내는 **특성 중요도** 계산해줌!!

결정 트리 모델의 **`feature_importances_`** 속성 이용!!

```python
# 특성 중요도 계산

print(dt.feature_importances_)

>>>
[0.12345626 0.86862934 0.0079144 ]
```

두 번째 특성인 당도가 제일 높고 알코올 도수, pH 순서네!!