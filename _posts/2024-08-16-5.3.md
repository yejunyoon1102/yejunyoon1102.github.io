---
layout: post
title: "머신러닝 : 5.3 트리의 앙상블"
date: 2024-08-16
categories: [Machine Learning, 5장]
---
# 5.3 트리의 앙상블

<problem : 어떤 머신러닝 알고리즘이 가장 좋은 성능을 가질려나..?>

### 정형 데이터와 비정형 데이터

- 정형 데이터 : 가지런히 정리되어있는 데이터들 (CSV나 데이터베이스, 엑셀에 저장 용이)
- 비정형 데이터 : 반대 개념, 텍스트 데이터, 사진, 음악 등

정형 데이터를 다룰 때 가장 뛰어난 성과를 내는 알고리즘 : **앙상블 학습**

비정형 데이터를 다룰 떄 가장 뛰어난 성과를 내는 알고리즘 : **신경망 알고리즘**

### 랜덤 포레스트

**랜덤 포레스트** : 앙상블 학습의 대표주자

→ 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만들어 각 결정 트리의 예측을 사용해 최종 예측을 만드는 알고리즘!!

![1.png](/assets/img/posts/ML/ML5.3/1.png)

랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만듦!!

→ 우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만듦 (**한 샘플이 중복되어 추출될 수도 있음**)

이렇게 만들어진 샘플을 **부트스트랩 샘플**이라고 함!!!

기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만듦

![2.png](/assets/img/posts/ML/ML5.3/2.png)

또한 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾음!!!

- 분류 모델인 **`RandomForestClassifier`**는 기본적으로 전체 특성 개수의 제곱근만큼 특성 선택
- 회귀 모델인 **`RandomForestRegressor`**는 전체 특성을 사용

![3.png](/assets/img/posts/ML/ML5.3/3.png)

랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련함!!

- 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼음
- 회귀일 때는 단순히 각 트리의 예측을 평균함

랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적 성능 가짐

```python
# 와인 데이터셋을 판다스로 불러오고 훈련 세트와 테스트 세트로 나눔

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

**`cross_validate()`** 함수를 사용해 교차 검증 수행

RandomForestClassifier는 기본적으로 100개의 결정 트리를 사용하므로 **`n_jobs`** 매개변수를 -1로 지정하여 모든 CPU 코어를 사용하는 것이 좋음!!!!!

**`return_train_score`** 매개변수를 **`True`**로 지정하면 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환함!!!(기본값 False)

→ 과대적합을 파악하는 데 용이함

```python
# 랜덤 포레스트 모델의 교차 검증 점수 확인

from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9973541965122431 0.8905151032797809
```

흠.. 훈련 세트에 다소 과대적합되었지만 일단 매개변수 조정하지 말고 그냥 가자…

랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수 모두 제공

- **`criterion`** : 분할 품질을 측정(**`’gini’`** / **`‘entropy’`**)
- **`max_depth`** : 트리의 최대 깊이 제한
- **`max_features`** : 분할에 고려할 최대 특성 수 지정
- **`min_samples_split`** : 노드를 분할하기 위해 필요한 최소 샘플 수 지정
- **`min_impurity_decrease`** : 노드를 분할하기 위해 필요한 최소 불순도 감소 지정
- **`min_samples_leaf`** : 리프 노드에 있어야 할 최소 샘플 수 지정

또한 결정 트리의 큰 장점 중 하나인 특성 중요도 계산!!

→ 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것!!

```python
# 랜덤 포레스트 모델 훈련시키고 특성 중요도 확인

rf.fit(train_input, train_target)
print(rf.feature_importances_)

>>>
[0.23167441 0.50039841 0.26792718]

# 5.1절 결정 트리에서의 특성 중요도

>>>
[0.12345626 0.86862934 0.0079144 ]
```

오..  당도의 중요도가 조금 감소하고 도수와 pH의 중요도가 조금 상승했네..!

→ 과대적합을 줄이고 일반화 성능을 높이는 데 도움!!

RandomForestClassifier에는 자체적으로 모델을 평가하는 점수 확인 가능!!

훈련 세트에서 부트스트랩 샘플을 만들어 결정 트리를 훈련시키는데, 이 때 부트스트랩 샘플에 포함되지 않고 남는 샘플(OOB 샘플) 사용해서 평가!!!! (마치 검증 세트의 역할)

**`oob_score=True`** 로 지정해 OOB 점수 확인 가능!!(기본값 False)

```python
# OOB 점수 확인

rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)

rf.fit(train_input, train_target)
print(rf.oob_score_)

>>>
0.8934000384837406
```

교차 검증에서 얻은 점수랑 비슷한걸..?

OOB 점수를 사용하면 교차 검증을 대신할 수 있어서 훈련 세트에 더 많은 샘플 사용 가능!!

### 엑스트라 트리

**엑스트라 트리**는 랜덤 포레스트와 매우 비슷하게 동작!!

기본적으로 100개의 결정 트리를 훈련함

결정 트리가 제공하는 대부분의 매개변수 지원

전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용

- 차이점!!

엑스트라 트리는 부트스트랩 샘플을 사용 X!!!!! → 결정 트리를 만들 때 훈련 세트 사용

대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할함

사이킷런의 **`ExtraTreesClassifier`** 클래스 활용!!

```python
# 엑스트라 트리 모델의 교차 검증 점수 확인

from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9974503966084433 0.8887848893166506
```

보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야함, 하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 장점!!

```python
# 엑스트라 트리 모델 훈련시키고 특성 중요도 확인

et.fit(train_input, train_target)
print(et.feature_importances_)

>>>
[0.20183568 0.52242907 0.27573525]
```

### 그레디언트 부스팅

**그레디언트 부스팅**은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법!!

사이킷런의 **`GradientBoostingClassifier`**는 기본적으로 깊이가 3 인 결정 트리를 100개 사용

→ 경사 하강법을 사용해서 트리를 앙상블에 추가함!!!

분류에서는 로지스틱 손실 함수, 회귀에서는 평균 제곱 오차 함수 사용

그레디언트 부스팅도 깊이가 얕은 결정 트리를 하나씩 추가하면서 산을 내려오는 느낌!!

→ **과대적합에 강하고** 일반적으로 높은 일반화 성능 기대!!

```python
# 그레디언트 부스팅 모델의 교차 검증 점수 확인

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.8881086892152563 0.8720430147331015
```

결정 트리의 개수를 늘려도 과대적합에 매우 강함!!!

**`n_estimators`** 매개변수로 결정트리 개수 지정(기본값 100)

**`learning_rate`** 매개변수로 학습률 지정(기본값 0.1)

```python
# 결정 트리의 개수 늘리고 학습률 늘려서 성능 향상 확인

gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9464595437171814 0.8780082549788999
```

```python
# 그레디언트 부스팅 모델 훈련시키고 특성 중요도 확인

gb.fit(train_input, train_target)
print(gb.feature_importances_)

>>>
[0.15872278 0.68010884 0.16116839]
```

일반적으로 그레디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능 기대!!

하지만 훈련 속도가 느리다는 단점(n_jobs 매개변수가 없어서)

### 히스토그램 기반 그레디언트 부스팅

**히스토그램 기반 그레디언트 부스팅**은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘!!!

먼저 입력 특성을 256개의 구간으로 나눔 → 노드를 분할할 때 최적의 분할 매우 빠르게 찾음

256개의 구간 중에서 하나를 떼어놓고 누락된 값을 위해서 사용 → 입력에 누락된 특성이 있어도 따로 전처리 할 필요 X!!

**`HistGradientBoostingClassifier`** 클래스 활용!!

일반적으로 기본 매개변수에서 안정적인 성능 기대

트리의 개수를 지정하는 데 n_estimators 대신 **`max_iter`** 사용!!

```python
# 히스토그램 기반 그레디언트 부스팅 모델의 교차 검증 점수 확인

from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9321723946453317 0.8801241948619236
```

```python
# 히스토그램 기반 그레디언트 부스팅 모델 훈련시키고 특성 중요도 확인

hgb.fit(train_input, train_target)
print(rf.feature_importaces_)

>>>
[0.23167441, 0.50039841, 0.26792718]
```

```python
# 히스토그램 기반 그레디언트 부스팅 모델의 테스트 세트에서의 성능 확인

hgb.score(test_input, test_target)

>>>
0.8723076923076923
```

사이킷런 말고도 다른 라이브러리에서도 히스토그램 기반 그레디언트 부스팅 알고리즘 사용 가능

```python
# XGBoost 라이브러리

from xgboost import XGBClassifier

xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9555033709953124 0.8799326275264677
```

```python
# LightGBM 라이브러리

from lightgbm import LGBMClassifier

lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.935828414851749 0.8801251203079884
```