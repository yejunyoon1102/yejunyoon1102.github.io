---
layout: post
title: "데이터 분석 : 파이썬 데이터 분석 실무 테크닉 100 1장"
date: 2024-09-17
categories: [Data Analysis, Technique]
---
# 1장 : 웹에서 주문 수를 분석하는 테크닉 10

topic : 어떤 기업 쇼핑몰 사이트의 상품 주문 수의 추세를 분석함으로써 판매량 개선의 방향 찾기!! 

→ 쇼핑몰 사이트 데이터는 비교적 ‘깨끗한’ 데이터임

![1.png](/assets/img/posts/DBS/DBS%201/1.png)

## 1. 데이터를 읽어 들이자

```python
# 고객 정보 데이터 

import pandas as pd

customer_master = pd.read_csv('customer_master.csv')
customer_master.head()
```

![2.png](/assets/img/posts/DBS/DBS%201/2.png)

```python
# 상품 데이터 

item_master = pd.read_csv('item_master.csv')
item_master.head()
```

![3.png](/assets/img/posts/DBS/DBS%201/3.png)

```python
# 구매내역 데이터

transaction_1 = pd.read_csv('transaction_1.csv')
transaction_1.head()
```

![4.png](/assets/img/posts/DBS/DBS%201/4.png)

```python
# 구매내역 상세 데이터

transaction_detail_1 = pd.read_csv('transaction_detail_1.csv')
transaction_detail_1.head()
```

![5.png](/assets/img/posts/DBS/DBS%201/5.png)

실무에서는 먼저 데이터를 수집한 후 데이터의 개요를 파악하고 나서 분석에 적절한 형태로 가공하는 경우가 많음!!!

**일단 데이터 전체를 파악하는 것이 중요**

되도록 상세하게 나와 있는 쪽에 맞추어 데이터를 가공!

transaction_detail을 기준으로 생각할 때, 

→ 첫 번째로 transaction_detail_1 과 transaction_detail_2, 그리고 transaction_1과 transaction_2 를 세로로 조인하는 것

→ 두 번째로 transaction_detail을 기준으로 transaction, customer_master, item_master를 가로로 조인하는 것

## 2. 데이터를 결합(조인) 해보자 - 세로 결합

```python
# 데이터 결합 (첫 번째 문제 - 세로 결합, transaction)

transaction_2 = pd.read_csv('transaction_2.csv')
transaction = pd.concat([transaction_1, transaction_2], ignore_index=True)
transaction.head()
```

**`pd.concat`** 을 사용해서 데이터 프레임 세로 결합

**`ignore_index=True`** : 기존의 인덱스를 무시하고, 새로 0부터 순차적으로 인덱스를 부여하여 데이터 프레임 결합 시 중복된 인덱스 발생하지 않음!

![6.png](/assets/img/posts/DBS/DBS%201/6.png)

```python
# 길이 확인

print(len(transaction_1))
print(len(transaction_2))
print(len(transaction))

>>>
5000
1786
6786
```

```python
# 데이터 결합(첫 번째 문제 - 세로 결합, transaction_detail)

transaction_detail_2 = pd.read_csv('transaction_detail_2.csv')
transaction_detail=pd.concat([transaction_detail_1,transaction_detail_2], ignore_index=True)
transaction_detail.head()
```

## 3. 매출 데이터끼리 결합(조인)해보자 - 가로 결합

가장 상세한 데이터인 transaction_detail을 기준 데이터로 결정

- 부족한 데이터 칼럼이 무엇인가?
- 공통되는 데이터 칼럼은 무엇인가?

→ 추가하고 싶은 데이터는 transaction의 payment_date, customer_id

price 열은 quantity x item_price로 나오니까 중복되서 계산되는 셈! 추가 X

→ 공통 데이터 칼럼은 transaction_id

```python
# 데이터 결합(두 번째 문제 - 가로 결합)

join_data = pd.merge(transaction_detail, transaction[["transaction_id", "payment_date", "customer_id"]], 
                     on="transaction_id", how="left")
join_data.head()
```

**`pd.merge`**를 사용하여 데이터 프레임 가로 결합, SQL에서 JOIN을 수행하는 것과 유사!

**`transaction_detail`** : 병합할 첫 번째 데이터프레임

**`transaction[["transaction_id", "payment_date", "customer_id"]]`** : 병합에 필요한 일부 열만 사용

**`on="transaction_id"`** : 두 데이터 프레임을 병합할 때 기준이 되는 열

**`how="left"`** : 병합 방식, left join을 의미!! → transaction_detail을 기준으로 병합을 수행

![7.png](/assets/img/posts/DBS/DBS%201/7.png)

```python
# 길이 확인

print(len(transaction_detail))
print(len(transaction))
print(len(join_data))

>>>
7144
6786
7144
```

→ 가로 결합했으니 길이는 같게 나오는 모습

## 4. 마스터 데이터를 결합(조인)해보자 - 가로 결합

- 부족한 데이터 칼럼이 무엇인가?

→ customer_master와 item_master에 포함된 데이터

- 공통되는 데이터 칼럼은 무엇인가?

→ customer_id, item_id로 연결

```python
# 데이터 결합(두 번째 문제 - 가로 결합)

join_data = pd.merge(join_data, customer_master, on="customer_id", how="left")
join_data = pd.merge(join_data, item_master, on="item_id", how="left")
join_data.head()
```

![8.png](/assets/img/posts/DBS/DBS%201/8.png)

이렇게 4종류의 데이터 프레임, 6개의 데이터를 결합해서 분석할 수 있는 하나의 데이터로 정리!! → 결합의 영향으로 price가 사라졌기 때문에 다시 계산 필요

## 5. 필요한 데이터 칼럼을 만들자

매출(price)는 quantity와 item_price의 곱을 계산해서 추가할 수 있음!!

```python
# 열 추가 이후 확인

join_data["price"] = join_data["quantity"] * join_data["item_price"]
join_data[["quantity", "item_price","price"]].head()
```

![9.png](/assets/img/posts/DBS/DBS%201/9.png)

→ 데이터 가공은 한 번 잘못하면 큰 에러 발생 가능, 따라서 **데이터 검산** 필요!!

## 6. 데이터를 검산하자

흠…

데이터 가공 전 transaction의 price 총합과 가공 후에 계산한 price의 총합은 같겠네!!

```python
# 각각 총합 구해서 비교하기

print(join_data["price"].sum())
print(transaction["price"].sum())

971135000
971135000
```

```python
# 각각 총합 구해서 비교하기(Boolean 방식)

join_data"price"].sum() == transaction["price"].sum()

>>>
True
```

## 7. 각종 통계량을 파악하자

데이터 분석을 진행할 때는 크게 두 가지 숫자를 파악해야 함!!

1. **결손치의 개수**
2. **전체를 파악할 수 있는 숫자감**

→ 결손치는 계산이나 머신러닝의 결과에 큰 영향을 미치기 때문에 숫자를 파악해두고 제거하거나 보간해야 함

```python
# 결손치의 개수를 출력

join_data.isnull().sum()

>>>
detail_id            0
transaction_id       0
item_id              0
quantity             0
payment_date         0
customer_id          0
customer_name        0
registration_date    0
email                0
gender               0
age                  0
birth                0
pref                 0
item_name            0
item_price           0
price                0
dtype: int64
```

**`isnull()`** 을 이용하면 결손치가 True/False로 반환됨!

```python
# 전체적인 느낌 파악을 위해 각종 통계량 출력

join_data.describe()
```

![10.png](/assets/img/posts/DBS/DBS%201/10.png)

해석) price를 보면 평균이 135937원 인데 max는 420000인데 대충 가장 비싼 210000짜리 pc를 2대 산 사람이 있겠구나~ 유추 가능!

```python
# 집계 기간 확인

print(join_data["payment_date"].min())
print(join_data["payment_date"].max())

>>>
2019-02-01 01:36:57
2019-07-31 23:41:38
```

## 8. 월별로 데이터를 집계해보자

전체 데이터를 한 번에 분석하면 데이터의 시계열 변화를 잘못 파악하는 경우가 있음

→ 데이터 범위를 좁혀서 분석하는 것도 하나의 방법!

구입일인 payment_date에서 연 월을 추출해서 새롭게 칼럼을 작성한 후 연 월 단위로 price를 집계해서 표시하자!

```python
# payment_date의 데이터 타입 확인

join_data.dtypes

>>>
detail_id                     int64
transaction_id               object
item_id                      object
quantity                      int64
payment_date                 object
customer_id                  object
customer_name                object
registration_date            object
email                        object
gender                       object
age                           int64
birth                        object
pref                         object
item_name                    object
item_price                    int64
price                         int64
payment_month                object
dtype: object
```

문자열 타입인 payment_date를 datetime형으로 바꿔볼까??

```python
# 데이터 타입 변경(문자열 -> 시계열)

join_data["payment_date"] = pd.to_datetime(join_data["payment_date"])
join_data["payment_month"] = join_data["payment_date"].dt.strftime("%Y%m")
join_data[["payment_date", "payment_month"]].head()
```

**`pd.to_datetime`** 함수 사용하여 시계열 타입으로 전환!

**`dt.strftime("%Y%m")`** :payment_date를 YYYYMM형식의 문자열로 변환하여 저장

ex) 2024-09-16 같은 날짜는 202409로 변환됨

```python
# 월 별로 그룹지어 매출 계산 

join_data.groupby("payment_month").sum()["price"]

>>>
payment_month
201902    160185000
201903    160370000
201904    160510000
201905    155420000
201906    164030000
201907    170620000
Name: price, dtype: int64
```

groupby는 집계하고 싶은 칼럼(payment_month)과 집계 방법(sum) 기술!!

마지막에 sum()으로 계산된 결과에서 price 열만 선택!

해석) 5월은 매출이 조금 내려갔지만 6, 7월은 회복!, 한 달에 대략 1억6천 정도의 매출, 연간 20억 원 정도의 매출 기대

## 9. 월별, 상품별로 데이터를 집계해보자

월별은 방금 했고, 이번에는 상품별로 같이 매출의 합계와 수량을 표시해보자!

```python
# 월 별, 상품 별로 그룹지어 매출과 수량 계산

join_data.groupby(["payment_month","item_name"]).sum()[["price", "quantity"]]
```

![11.png](/assets/img/posts/DBS/DBS%201/11.png)

groupby에서 출력하고 싶은 칼럼이 여러 개 있을 경우, 리스트형으로 지정하면 됨

출력 결과가 직관적으로 이해하기 어려우니 **`pivot_table`**을 사용해 다시 집계!!

```python
# pivot_table 이용

pd.pivot_table(join_data, index='item_name', columns='payment_month', values=['price', 'quantity'], aggfunc='sum')
```

![12.png](/assets/img/posts/DBS/DBS%201/12.png)

**`pivot_table`**은 데이터를 특정 기준에 따라 재구성하여 요약하고, 각 그룹에 대해 집계 연산 수행

**`index='item_name’`** : 행 인덱스가 되는 열을 지정

**`columns='payment_month’`** : 열을 그룹화할 기준 설정

→ 각 월들이 열로 표시되고, 해당 월에 대한 값들이 상품명에 따라 집계됨

values=['price', 'quantity'] : 피벗 테이블에 표시될 값이 되는 열 지정

→ 각 상품명과 월별 조합에 대해 매출과 수량 열의 값을 보여줌

**`aggfunc='sum’`** : 집계 방식 지정

해석) 매출의 합계는 PC-E가 가장 높지만, 수량에서는 가장 싼 PC-A가 많네… 앞에 월별 추세에서 5월에 매출이 감소하고 6, 7 월에 증가한 걸 보고, 여기서 5월에 PC-E의 매출이 감소한 게 영향을 미친 것 같네!!

## 10. 상품별 매출 추이를 가시화해 보자

```python
# 그래프 그릴 pivot_table 다시 지정

graph_data = pd.pivot_table(join_data, index='payment_month', columns='item_name', values='price', aggfunc='sum')
graph_data.head()
```

행을 월별로, 열을 상품별로 지정하고, 매출만 표시하게 수정

![13.png](/assets/img/posts/DBS/DBS%201/13.png)

```python
# 그래프 시각화

import matplotlib.pyplot as plt

plt.plot(list(graph_data.index), graph_data["PC-A"], label='PC-A')
plt.plot(list(graph_data.index), graph_data["PC-B"], label='PC-B')
plt.plot(list(graph_data.index), graph_data["PC-C"], label='PC-C')
plt.plot(list(graph_data.index), graph_data["PC-D"], label='PC-D')
plt.plot(list(graph_data.index), graph_data["PC-E"], label='PC-E')
plt.legend()  
```

![14.png](/assets/img/posts/DBS/DBS%201/14.png)

이렇게 시각화 하면 PC-E가 매출을 견인하는 기종이라는 점과 매출 추이를 파악할 수 있네!!