---
layout: post
title: "데이터 분석 : 12장 통계 기반 분석 방법론"
date: 2024-11-04
categories : [Data Analysis, 12장]
---
# 12장 : 통계 기반 분석 방법론

### 12.1 분석 모델 개요

![1.png](/assets/img/posts/DBA/DBA12/1.png)

![2.png](/assets/img/posts/DBA/DBA12/2.png)

![3.png](/assets/img/posts/DBA/DBA12/3.png)

기계 학습 데이터 분석 방법론

- 지도 학습 : 종속변수 존재 (데이터에 정답 포함)
    
    결과값이 양적 척도면 회귀 방식 / 결과값이 질적 척도면 분류 방식
    
- 비지도 학습 : 종속변수 존재 X (데이터에 정답이 없는 상태에서 패턴 찾기)
- 강화 학습 : 종속변수 존재 X (상호작용을 통해 보상을 받으며 학습)

### 12.2 주성분 분석(PCA)

전체 변수들의 핵심 특성만 선별 → 독립변수(차원)의 수를 줄일 수 있음

- 주성분 분석(PCA) : 변수의 수를 축약하면서 정보의 손실을 최소화
- 공통요인 분석(CFA) : 변수들 사이에 존재하는 차원을 규명함으로써 변수간의 구조 파악

전체 분산 중에서 해당 주성분이 갖고 있는 분산이 곧 설명력!!

일반적으로 2개의 주성분으로 설명이 다 되긴 함

```python
# 필요한 패키지 설치
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 300
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/glass.csv")

# 데이터 샘플 확인
df.head()
```

![4.png](/assets/img/posts/DBA/DBA12/4.png)

```python
# 데이터 스케일 정규화

# 차원축소할 칼럼만 선택 (종속변수 Type 제거)
df1 = df.drop('Type', axis=1)

# 데이터 정규화 적용
MinMaxScaler = MinMaxScaler()
df_minmax = MinMaxScaler.fit_transform(df1)

# 컬럼명 결합
df_minmax = pd.DataFrame(data=df_minmax, columns=df1.columns)

df_minmax.head()
```

![5.png](/assets/img/posts/DBA/DBA12/5.png)

```python
# 주성분 개수 설정(최대 수 설정)
pca = PCA(n_components=9) 
df_pca = pca.fit_transform(df_minmax)

# 주성분으로 변형된 테이블 생성
df_pca = pd.DataFrame(data=df_pca, columns = ['C1','C2','C3','C4','C5','C6','C7','C8','C9'])

# 주성분 설명력 확인
np.round_(pca.explained_variance_ratio_,3)

>>>
array([0.454, 0.18 , 0.126, 0.098, 0.069, 0.042, 0.026, 0.004, 0.   ])
```

9개 주성분의 설명력 총 합은 100%, C1, C2의 주성분으로 약 63%의 설명력 가짐

```python
# 주성분 개수 설정(2개 설정)
pca = PCA(n_components=2) 
df_pca = pca.fit_transform(df_minmax)

# 주성분으로 변형된 테이블 생성
df_pca = pd.DataFrame(data=df_pca, columns = ['C1','C2'])

df_pca.head()
```

![6.png](/assets/img/posts/DBA/DBA12/6.png)

이로써 기존의 9개의 변수를 2개로 차원축소

```python
# 주성분에 따른 종속변수 시각화

# 주성분 테이블에 종속변수 칼럼 결합
df_concat = pd.concat([df_pca,df[['Type']]],axis=1)

# 산점도 시각화
sns.scatterplot(data=df_concat,x='C1',y='C2',hue='Type')
```

![7.png](/assets/img/posts/DBA/DBA12/7.png)

제1 주성분과 제2 주성분을 x축과 y축으로 하여 종속변수인 Type이 어떻게 분포하는지 확인

### 12.3 공통요인분석(CFA)

PCA와 CFA는 요인분석을 하기 위한 기법의 종류!!

요인분석을 하기 위해 전체 분산을 토대로 요인을 추출하는 PCA를 사용하거나 공통분산만을 토대로 요인을 추출하는 CFA 선택 가능

CFA는 상관성이 높은 변수들을 묶어 잠재된 몇 개의 변수를 찾는다는 점에서 차이

정확성 측면, 공통분산을 가진 변수들이 많을 때엔  CFA가 더 우수!!

요인분석을 하기 위해서는 우선 독립변수들 간의 상관성이 요인분석에 적합한지 검증

- 바틀렛 테스트
    
    행렬식을 이용하여 카이제곱값을 구하여 각 변수들 사이의 상관계수의 적합성 검증
    
    유의확률 p가 0.05보다 작으면 분석에 적합하다고 판단!
    
- KMO 검정
    
    변수들 간의 상관관계가 다른 변수에 의해 잘 설명되는 정도를 나타내는 값을 통계적으로 산출
    
    0.8 이상이면 우수, 0.5 이상이면 적합, 0.5 미만이면 부적합
    

주성분 변수들의 고유치를 확인하여 요인 개수 결정

고유치 = 요인이 설명해주는 분산의 양, 요인에 해당하는 변수들의 요인 적재 값의 제곱 합 값들을 합하여 구함

누적된 설명력 & 엘보우 포인트 활용하여 요인 개수 선택

```python
# 필요한 패키지 설치
!pip install factor-analyzer
from sklearn.preprocessing import MinMaxScaler
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
from factor_analyzer.factor_analyzer import calculate_kmo
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 300
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/nba2021_advanced.csv")

# 데이터 샘플 확인
df.head()
```

![8.png](/assets/img/posts/DBA/DBA12/8.png)

```python
# 변수 선택 및 데이터 스케일 정규화

# 요인 분석 할 칼럼만 선택 (문자형 변수 등 제거)
df1 = df.drop(['Player','Pos','Tm','Age'], axis='columns')

# 데이터 정규화 적용
MinMaxScaler = MinMaxScaler()
df_minmax = MinMaxScaler.fit_transform(df1)

# 컬럼명 결합
df_minmax = pd.DataFrame(data=df_minmax, columns=df1.columns)

df_minmax.head()
```

![9.png](/assets/img/posts/DBA/DBA12/9.png)

```python
# 바틀렛(Bartlett)테스트
chi_square_value,p_value=calculate_bartlett_sphericity(df_minmax)

# 카이스퀘어, P-value 확인
chi_square_value, p_value

>>>
(20195.698680942383, 0.0)
```

p-value가 0.05보다 작으므로 공통요인분석에 적합

```python
# KMO 검정
kmo_all,kmo_test=calculate_kmo(df_minmax)
kmo_test

>>>
0.7008950495577879
```

0.7은 적합 수준이므로 공통요인분석에 적합

```python
# 전체 요인적재값 확인

fa = FactorAnalyzer(n_factors=30,rotation=None)
fa.fit(df_minmax)
ev, v = fa.get_eigenvalues()

np.round_(ev,2)

>>>
array([7.91, 3.53, 2.25, 1.59, 1.45, 1.05, 0.86, 0.67, 0.59, 0.57, 0.45,
       0.37, 0.24, 0.17, 0.11, 0.08, 0.07, 0.03, 0.01, 0.  , 0.  , 0.  ])
```

```python
# 스크리 도표 시각화

plt.scatter(range(1,df_minmax.shape[1]+1),ev)
plt.plot(range(1,df_minmax.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()
```

![10.png](/assets/img/posts/DBA/DBA12/10.png)

엘보우 포인트 약 4개 지점!

```python
# 요인 수 선정 및 요인-변수 간 요인적재값(상관관계) 확인

fa = FactorAnalyzer(n_factors=4, rotation="varimax")
fa.fit(df_minmax)
factor_loadings = pd.DataFrame(fa.loadings_, index=df_minmax.columns)
factor_loadings
```

![11.png](/assets/img/posts/DBA/DBA12/11.png)

```python
# 변수 간 요인적재값 시각화
plt.figure(figsize=(6,10))
sns.heatmap(abs(factor_loadings), cmap="PuBu", annot=True, fmt='.2f')
```

### 12.4 다중공선성 해결과 섀플리 밸류 분석

**다중공선성 판별 기준**

- 상관분석을 통해 독립 변수 간의 상관성을 확인하여 높은 상관계수를 갖는 독립변수 확인
    
    절대치가 0.7 이상이면 다중공선성 의심
    
- 독립변수들의 설명력을 의미하는 결정계수 R²은 크지만 회귀계수에 대한 t값이 낮다면 다중공선성 의심
- VIF(분산팽창계수)를 통해 판단

![12.png](/assets/img/posts/DBA/DBA12/12.png)

VIF가 크다는 것은 해당 변수가 다른 변수들과 상관성이 높다는 것 → 제거

일반적으로 5 이상이면 다중공선성 의심, 10 이상이면 다중공선성이 있다고 판단

**다중공선성 해결 방법**

- 다중공선성을 해결하기 위한 가장 기본적인 방법은 VIF값이 높은 변수들 중에서 종속변수와의 상관성(설명력)이 가장 낮은 변수를 제거하고 다시 VIF값을 확인하는 것을 반복하는 것!!
- 변수를 가공(로그, 표준화 및 정규화)
- 주성분 분석
- 변수 선택 알고리즘(전진 선택법, 후진 제거법, 단계적 선택법)

**섀플리 밸류 분석**

각 독립변수가 종속변수의 설명력에 기여하는 순수한 수치를 계산하는 방법

![13.png](/assets/img/posts/DBA/DBA12/13.png)

x1 의 설명력 = 0.15

x1, x2 조합에서 x1의 기여도 = 0.32 - 0.21 = 0.11

x1, x3 조합에서 x1의 기여도 = 0.29 - 0.17 = 0.12

x1, x2, x3 조합에서 x1의 기여도 = 0.35 - 0.31 = 0.04

1개 조합 = 0.15 / 2개 조합 = 0.115 / 3개 조합 = 0.04

평균(섀플리 밸류) = 0.101 → x1, x2, x3을 모두 투입했을 때 x1이 갖는 순수한 설명력

### 12.5 데이터 마사지와 블라인드 분석

**데이터 마사지** 

데이터 분석 결과가 예상하거나 의도한 방향과 다를 때 수정하기

- 편향된 데이터 전처리
    
    각 그룹의 이상치나 결측값을 의도한 방향에 유리하도록 처리
    
- 매직 그래프 사용
    
    그래프의 레이블 간격이나 비율을 왜곡하기
    
- 분모 바꾸기 등 관점 변환
    
    동일한 비율 차이라 하더라도 분모를 어떻게 설정하는가에 따라 받아들여지는 느낌이 다름
    
- 의도적인 데이터 누락 및 가공
    
    원하는 방향과 반대되는 데이터를 의독적으로 누락시키거나 다른 수치와 결합하여 특성 오나화
    
- 머신러닝 모델의 파라미터 값 변경 및 연산반복
    
    모델의 파라미터 값을 변경해가며 다양하게 연산 반복
    
- 심슨의 역설
    
    데이터의 세부 비중에 따라 전체 대표 확률이 왜곡되는 현상을 의도적으로 적용하여 통계 수치를 실제와는 정 반대로 표현
    

**블라인드 분석**

데이터 마사지에 의한 왜곡을 방지

기존에 분석가가 중요하다고 생각했던 변수가 큰 의미가 없는 것으로 결과가 나왔을 때 무리해서 의미부여를 하거나 그 변수에 집착해서 해석에 유리하도록 변수를 가공하게 되는 실수 방지 목적!!

### 12.6 Z-test와 T-test

두 집단간의 평균차이가 우연적인지, 정말 통계적으로 유의미한 차이인지

- Z-test : 표본의 크기가 30 이상 → 중심 극한정리
    
    ![14.png](/assets/img/posts/DBA/DBA12/14.png)
    

σ : 모집단의 표준편차

- T-test : 표본의 크기가 30 미만

    ![15.png](/assets/img/posts/DBA/DBA12/15.png)

```python
# 필요한 패키지 설치

from scipy.stats import shapiro
from statsmodels.stats.weightstats import ztest as ztest
import scipy.stats
from scipy import stats
from scipy.stats import ttest_ind
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.rcParams['figure.dpi'] = 300
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/Golf_test.csv")

# 데이터 샘플 확인
df.head()
```

![16.png](/assets/img/posts/DBA/DBA12/16.png)

```python
# 골프공 타입 별 통계값 확인

df.describe()
```

![17.png](/assets/img/posts/DBA/DBA12/17.png)

```python
# 그룹 별 박스 플롯 시각화

df2 =  pd.melt(df)
plt.figure(figsize=(12,6))
sns.boxplot(x='variable', y='value', data=df2)
plt.title('Golf ball test')
plt.show()
```

![18.png](/assets/img/posts/DBA/DBA12/18.png)

```python
# 데이터 정규성 검정

print(shapiro(df['TypeA_before']))
print(shapiro(df['TypeA_after']))
print(shapiro(df['TypeB_before']))
print(shapiro(df['TypeB_after']))
print(shapiro(df['TypeC_before']))
print(shapiro(df['TypeC_after']))

>>>
ShapiroResult(statistic=0.965537965297699, pvalue=0.15155252814292908)
ShapiroResult(statistic=0.9728281497955322, pvalue=0.3005155026912689)
ShapiroResult(statistic=0.9730034470558167, pvalue=0.30533552169799805)
ShapiroResult(statistic=0.9693009257316589, pvalue=0.2167525738477707)
ShapiroResult(statistic=0.9595518112182617, pvalue=0.08513051271438599)
ShapiroResult(statistic=0.9469832181930542, pvalue=0.02568192593753338)
```

샤피로-윌크 검정을 통해 p-value가 0.05를 초과하는지 판단

TypeC_after을 제외하고는 모두 0.05를 초과하기 때문에 정규성을 가지지 않는다는 귀무가설 기각 → 정규성 만족!

```python
# 데이터 등분산성 검정
stats.bartlett(df['TypeA_before'],df['TypeA_after'],
               df['TypeB_before'],df['TypeB_after'],
               df['TypeC_before'],df['TypeC_after'])
               
>>>
BartlettResult(statistic=2.232358764161634, pvalue=0.8161484218330712)
```

6개의 칼럼에 대해 p-value가 0.05 이상이므로 등분산성이라는 귀무가설 채택

```python
# Z-test (TypeA_Before와 TypeA_After)
ztest(df['TypeA_before'], x2=df['TypeA_after'], value=0, alternative='two-sided')

>>>
(-1.2313987682249838, 0.21817375744980994)
```

두 번째 수치인 p-value가 0.218로 유의하지 않음 → TypeA 골프공은 특정 처리를 하기 전과 후의 비거리가 통계적으로 차이가 없다!!

```python
# Z-test (TypeA_Before와 TypeB_before)

# 양측검정
print(ztest(df['TypeA_before'], x2=df['TypeB_before'], value=0, alternative='two-sided'))

# 단측검정(왼꼬리검정)
print(ztest(df['TypeA_before'], x2=df['TypeB_before'], value=0, alternative='smaller'))

# 단측검정(오른꼬리검정)
print(ztest(df['TypeA_before'], x2=df['TypeB_before'], value=0, alternative='larger'))

>>>
(-2.789495974658115, 0.005279015267745852)
(-2.789495974658115, 0.002639507633872926)
(-2.789495974658115, 0.9973604923661271)
```

```python
# 대응표본 t검정 수행
scipy.stats.ttest_rel(df['TypeA_before'],df['TypeA_after'])

>>>
Ttest_relResult(statistic=-1.221439914972903, pvalue=0.22776376448687602)
```

```python
# 독립표본 t검정 수행
ttest_ind(df['TypeA_before'],df['TypeB_before'], equal_var=False)

>>>
Ttest_indResult(statistic=-2.789495974658115, pvalue=0.0063642243501398285)
```

### 12.7 ANOVA(Analysis Of Variance)

세 집단 이상의 평균을 검정할 때는 ANOVA 사용!

F분포 사용

- 귀무가설 : 독립변수(인자)의 차이에 따른 종속변수(특성 값)는 동일하다
- 대립가설 : 독립변수(인자)의 차이에 따른 종속변수(특성 값)는 다르다

![19.png](/assets/img/posts/DBA/DBA12/19.png)

![20.png](/assets/img/posts/DBA/DBA12/20.png)

```python
# 필요한 패키지 설치

from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scipy.stats
import scipy.stats as stats
from scipy.stats import ttest_ind
import pandas as pd
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/Golf_test.csv")

# 데이터 샘플 확인
df.head()
```

![21.png](/assets/img/posts/DBA/DBA12/21.png)

```python
# stats 패키지 아노바 검정

F_statistic, pVal = stats.f_oneway(df['TypeA_before'], 
                                   df['TypeB_before'], 
                                   df['TypeC_before'])

print('일원분산분석 결과 : F={0:.1f}, p={1:.5f}'.format(F_statistic, pVal))

>>>
일원분산분석 결과 : F=4.2, p=0.01652
```

p-value가 0.016이므로 3개 변수 중 최소한 하나의 변수 조합 간에는 차이가 있네!

```python
# 데이터 재구조화

df2 =  pd.melt(df)
df2 = df2[df2['variable'].isin(['TypeA_before', 'TypeB_before', 'TypeC_before'])]

df2.head()
```

melt() 함수를 사용하여 구분자인 variable 칼럼과 실젯값이 담긴 value 칼럼으로 재구조화

![22.png](/assets/img/posts/DBA/DBA12/22.png)

```python
# ols 패키지 아노바 검정

model = ols('value ~ C(variable)', df2).fit()
print(anova_lm(model))

>>>
                df        sum_sq     mean_sq         F    PR(>F)
C(variable)    2.0    675.453333  337.726667  4.220169  0.016515
Residual     147.0  11763.940000   80.026803       NaN       NaN
```

```python
# 사후검정

posthoc = pairwise_tukeyhsd(df2['value'], 
                            df2['variable'], 
                            alpha=0.05)
print(posthoc)
fig = posthoc.plot_simultaneous()
```

![23.png](/assets/img/posts/DBA/DBA12/23.png)

사후검정으로 어떤 변수들 간에 유의미한 차이가 있는지 확인

TypeA_before과 TypeB_before 간에만 유의미한 차이가 있는 것으로 확인

### 12.8 카이제곱 검정(교차분석)

명목 혹은 서열척도와 같은 범주형 변수들 간의 연관성을 분석하기 위해 결합분포 활용

카이제곱 값을 통해 변수 간에 연관성이 없다는 귀무가설을 기각하는지 여부로 상관성이 있고 없음을 판단

```python
# 필요한 패키지 설치

import scipy.stats
from scipy.stats import chi2_contingency
import pandas as pd
import matplotlib.pyplot as plt
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/smoker.csv")

# 데이터 샘플 확인
df.head()
```

![24.png](/assets/img/posts/DBA/DBA12/24.png)

```python
# 항목 별 집계

df.groupby(['sex','smoke'])['smoke'].count()

>>>
sex     smoke     
female  Non-Smoker    50
        Smoker        12
male    Non-Smoker    40
        Smoker        29
Name: smoke, dtype: int64
```

```python
# 카이제곱 검정용 데이터셋 가공

crosstab = pd.crosstab(df.sex, df.smoke)
crosstab
```

![25.png](/assets/img/posts/DBA/DBA12/25.png)

```python
# 성별 별 흡연자 수 시각화

%matplotlib inline
crosstab.plot(kind='bar', figsize=(10,5))
plt.grid()
```

![26.png](/assets/img/posts/DBA/DBA12/26.png)

```python
# 카이제곱 검정

chiresult = chi2_contingency(crosstab, correction=False)
print('Chi square: {}'.format(chiresult[0]))
print('P-value: {}'.format(chiresult[1]))

>>>
Chi square: 7.8081404703715105
P-value: 0.005201139711454792
```

p-vlaue가 0.05보다 작으므로 성별에 따른 흡연자 비율 차이가 통계적으로 유의함