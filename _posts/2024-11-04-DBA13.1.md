---
layout: post
title: "데이터 분석 : 13장 선형회귀분석과 Elastic Net(예측모델)"
date: 2024-11-04
categories : [Data Analysis, 13장]
---
# 13.1 선형 회귀분석과 Elastic Net(예측모델)

### 13.1.3 Ridge와 Lasso 그리고 Elastic Net

- Ridge : 전체 변수를 모두 유지하면서 각 변수의 계수 크기를 조정
    
    종속변수 예측에 영향을 거의 미치지 않는 변수는 0에 가까운 가중치를 주게 하여 독립변수들의 영향력을 조정
    
    → 다중공선성을 방지하면서 모델의 설명력을 최대화
    
- Lasso : Ridge와 유사하지만, 중요한 몇 개의 변수만 선택하고 나머지 변수들은 계수를 0으로 주어 변수의 영향력을 아예 없앰
- Elastic Net : 릿지와 라쏘의 정규화 항을 결합하여 절충
    
    Ridge는 변환된 계수가 0이 될 수 없지만 Lasso는 0이 될 수 있다는 특성 결합
    
    혼합비율(r)을 조절하여 성능을 최적으로!
    
    혼합비율이 0에 가까울수록 Ridge, 1에 가까울수록 Lasso
    
    독립변수를 이미 잘 정제해서 중요할 것으로 판단되는 변수들만 모델에 넣은 상태면 Ridge의 비율 높임, 변수 선택 없이 주어진 독립변수를 모두 집어넣은 상태라면 Lasso의 비율 높임
    

![1.png](/assets/img/posts/DBA/DBA13.1/1.png)

1. 모델에 대한 유의도
2. 모델 설명력
3. 수정된 모델 설명력

![2.png](/assets/img/posts/DBA/DBA13.1/2.png)

```python
# 필요한 패키지 설치

import pandas as pd # csv file 
import numpy as np
from sklearn.linear_model import LinearRegression # 선형회귀모델 생성 
from sklearn.model_selection import train_test_split # train/test set 생성 
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error # MSE : 평균제곱오차 - model 평가 
from sklearn import datasets # sklearn 기본 데이터셋 load
import seaborn as sns
import matplotlib.pyplot as plt
```

```python
# 데이터 불러오기
df = pd.read_csv("datasets/kc_house_data.csv")

# 데이터 샘플 확인
df.head()
```

![3.png](/assets/img/posts/DBA/DBA13.1/3.png)

```python
# 각 컬럼의 속성 및 결측치 확인
df.info()

>>>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 21613 entries, 0 to 21612
Data columns (total 21 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   id             21613 non-null  int64  
 1   date           21613 non-null  object 
 2   price          21613 non-null  float64
 3   bedrooms       21613 non-null  int64  
 4   bathrooms      21613 non-null  float64
 5   sqft_living    21613 non-null  int64  
 6   sqft_lot       21613 non-null  int64  
 7   floors         21613 non-null  float64
 8   waterfront     21613 non-null  int64  
 9   view           21613 non-null  int64  
 10  condition      21613 non-null  int64  
 11  grade          21613 non-null  int64  
 12  sqft_above     21613 non-null  int64  
 13  sqft_basement  21613 non-null  int64  
 14  yr_built       21613 non-null  int64  
 15  yr_renovated   21613 non-null  int64  
 16  zipcode        21613 non-null  int64  
 17  lat            21613 non-null  float64
 18  long           21613 non-null  float64
 19  sqft_living15  21613 non-null  int64  
 20  sqft_lot15     21613 non-null  int64  
dtypes: float64(5), int64(15), object(1)
memory usage: 3.5+ MB
```

```python
# 각 컬럼의 통계치 확인
df.describe()
```

![4.png](/assets/img/posts/DBA/DBA13.1/4.png)

```python
# 데이터 시각화 하여 분포 확인하기

import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 300

sns.pairplot(df[["price", "sqft_living", "sqft_basement", "yr_built", "zipcode"]])
plt.show()
```

![5.png](/assets/img/posts/DBA/DBA13.1/5.png)

오른쪽 대각선으로 시각화된 것들 → 강한 상관관계가 있는것

```python
# 변수 선택 및 컬럼명 붙여넣기 위한 컬럼 리스트 생성
df.columns

>>>
Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15'],
      dtype='object')
```

회귀모델 학습을 위한 데이터 가공

```python
# 독립변수와 종속변수 분리하여 생성
x = df[[ 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15']]
# 'id', 'date'는 키값에 해당하므로 변수에서 제외 해준다.
y = df[['price']]

# 학습셋과 테스트셋 분리하여 생성(7:3)
# df_train, df_test = train_test_split(df, test_size = 0.3) 
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.3)
```

```python
# 학습셋과 검증셋이 잘 나뉘었는지 확인
print('train data 개수: ', len(x_train))
print('test data 개수: ', len(x_test))

>>>
train data 개수:  15129
test data 개수:  6484
```

```python
# 다중회귀모델 생성
mreg = LinearRegression(fit_intercept=True)
mreg.fit(x_train, y_train) 

# 테스트셋에 모델 적용
y_predict = mreg.predict(x_test)

print("학습셋 R-Square: {:.2f}".format(mreg.score(x_train, y_train)))
print("테스트셋 R-Square: {:.2f}".format(mreg.score(x_test, y_test)))

>>>
학습셋 R-Square: 0.70
테스트셋 R-Square: 0.68
```

R-Square가 매우 준수한 모습

```python
# 모델의 상수값 확인
print(mreg.intercept_)

# 모델의 회귀계수 확인
print(mreg.coef_)

>>>
[5905170.26323962]
[[-4.04278815e+04  4.41345118e+04  1.12285390e+02  1.33649230e-01
   6.39986628e+03  5.71220651e+05  4.76076256e+04  2.68327180e+04
   9.66052229e+04  7.10823539e+01  4.12030351e+01 -2.65613516e+03
   1.57961389e+01 -5.72702851e+02  6.01491770e+05 -2.14480305e+05
   1.73045742e+01 -3.50042299e-01]]
```

```python
ols_m = sm.OLS(y_train, sm.add_constant(x_train)).fit()
ols_m.summary()
```

![6.png](/assets/img/posts/DBA/DBA13.1/6.png)

![7.png](/assets/img/posts/DBA/DBA13.1/7.png)

```python
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.16e-20. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
```

2번째 노트를 보면 다중공선성 문제가 있을 가능성이 높다고 알려줌

```python
# 다항회귀 불러오기
from sklearn.preprocessing import PolynomialFeatures

# 다항 변수 변환
poly_m = PolynomialFeatures(degree=2, include_bias=False)
x_train_poly = poly_m.fit_transform(x_train)

# 다항 회귀모델 생성
mreg_poly = LinearRegression(fit_intercept=True)

# 다항회귀 학습
mreg_poly.fit(x_train_poly, y_train) 

# 테스트셋에 모델 적용
x_test_poly = poly_m.fit_transform(x_test)
y_predict_poly = mreg_poly.predict(x_test_poly)
```

```python
print("학습셋 R-Square: {:.2f}".format(mreg_poly.score(x_train_poly, y_train)))
print("테스트셋 R-Square: {:.2f}".format(mreg_poly.score(x_test_poly, y_test)))

>>>
학습셋 R-Square: 0.83
테스트셋 R-Square: 0.82
```

```python
# Ridge 설치
from sklearn.linear_model import Ridge

# alpha 별 모델 생성
ridge = Ridge().fit(x_train, y_train)
ridge001 = Ridge(alpha=0.01).fit(x_train, y_train)
ridge100 = Ridge(alpha=100).fit(x_train, y_train)

# 모델 별 R-Square 산출
print("ridge_train R2: {:.3f}".format(ridge.score(x_train, y_train)))
print("ridge_test R2: {:.3f}".format(ridge.score(x_test, y_test)))

print("ridge001_train R2: {:.3f}".format(ridge001.score(x_train, y_train)))
print("ridge001_test R2: {:.3f}".format(ridge001.score(x_test, y_test)))

print("ridge100_train R2: {:.3f}".format(ridge100.score(x_train, y_train)))
print("ridge100_test R2: {:.3f}".format(ridge100.score(x_test, y_test)))

>>>
ridge_train R2: 0.701
ridge_test R2: 0.697
ridge001_train R2: 0.701
ridge001_test R2: 0.697
ridge100_train R2: 0.692
ridge100_test R2: 0.688
```

Ridge 회귀 모델의 기본 alpha 값은 1!!

추가적으로 alpha값을 0.01, 100으로 조정하여 R-square 비교 → 100으로 설정할 경우 과소적합 이후에 성능이 떨어짐

```python
# Lasso 설치
from sklearn.linear_model import Lasso

# alpha 별 모델 생성
lasso = Lasso().fit(x_train, y_train)
lasso001 = Lasso(alpha=0.01).fit(x_train, y_train)
lasso10000 = Lasso(alpha=10000).fit(x_train, y_train)

# 모델 별 R-Square 산출
print("lasso_train R2: {:.3f}".format(lasso.score(x_train, y_train)))
print("lasso_test R2: {:.3f}".format(lasso.score(x_test, y_test)))
print("num_of_IV:", np.sum(lasso.coef_ !=0))

print("lasso001_train R2: {:.3f}".format(lasso001.score(x_train, y_train)))
print("lasso001_test R2: {:.3f}".format(lasso001.score(x_test, y_test)))
print("num_of_IV:", np.sum(lasso001.coef_ !=0))

print("lasso10000_train R2: {:.3f}".format(lasso10000.score(x_train, y_train)))
print("lasso10000_test R2: {:.3f}".format(lasso10000.score(x_test, y_test)))
print("num_of_IV:", np.sum(lasso10000.coef_ !=0))

>>>
lasso_train R2: 0.701
lasso_test R2: 0.697
num_of_IV: 18
lasso001_train R2: 0.701
lasso001_test R2: 0.697
num_of_IV: 18
lasso10000_train R2: 0.628
lasso10000_test R2: 0.629
num_of_IV: 13
```

alpha를 10000으로 설정하면 5개의 변수가 탈락되어 총 13개의 변수가 사용된 모델 생성

```python
# ElasticNet 설치
from sklearn.linear_model import ElasticNet

# alpha 별 모델 생성
elast = ElasticNet().fit(x_train, y_train)
elast001 = ElasticNet(alpha=100, l1_ratio = 0.1).fit(x_train, y_train)
elast10000 = ElasticNet(alpha=10000, l1_ratio = 1).fit(x_train, y_train)

# 모델 별 R-Square 산출
print("elast_train R2: {:.3f}".format(elast.score(x_train, y_train)))
print("lasso_test R2: {:.3f}".format(elast.score(x_test, y_test)))
print("num_of_IV:", np.sum(elast.coef_ !=0))

print("elast001_train R2: {:.3f}".format(elast001.score(x_train, y_train)))
print("elast001_test R2: {:.3f}".format(elast001.score(x_test, y_test)))
print("num_of_IV:", np.sum(elast001.coef_ !=0))

print("elast10000_train R2: {:.3f}".format(elast10000.score(x_train, y_train)))
print("elast10000_test R2: {:.3f}".format(elast10000.score(x_test, y_test)))
print("num_of_IV:", np.sum(elast10000.coef_ !=0))

>>>
elast_train R2: 0.619
lasso_test R2: 0.619
num_of_IV: 18
elast001_train R2: 0.544
elast001_test R2: 0.547
num_of_IV: 18
elast10000_train R2: 0.628
elast10000_test R2: 0.629
num_of_IV: 13
```

ElasticNet은 기본적으로 alpha와 l1_ratio설정을 해줌

ElasticNet의 정규화는 Ridge와 Lasso를 합친 것으로 αL1 + βL2

l1_ratio는 0에서 1의 값을 가지며, Lasso 모델의 비중을 나타냄